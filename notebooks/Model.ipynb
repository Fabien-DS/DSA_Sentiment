{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "extreme-replica",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "previous-particular",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/raw/sample_submission.csv',\n",
       " '../data/raw/test.csv',\n",
       " '../data/raw/train.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_folder = os.path.join('..', 'data', 'raw')\n",
    "all_recipe_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)]\n",
    "all_recipe_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "subsequent-drill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission\n",
      "test\n",
      "train\n"
     ]
    }
   ],
   "source": [
    "documents = {}\n",
    "for text_fname in all_recipe_files:\n",
    "    bname = os.path.basename(text_fname).split('.')[0]\n",
    "    print(bname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sunrise-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = {}\n",
    "for text_fname in all_recipe_files:\n",
    "    bname = os.path.basename(text_fname).split('.')[0]\n",
    "    documents[bname] = pd.read_csv(text_fname, encoding='utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alpine-caribbean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_submission':           textID  selected_text\n",
       " 0     f87dea47db            NaN\n",
       " 1     96d74cb729            NaN\n",
       " 2     eee518ae67            NaN\n",
       " 3     01082688c6            NaN\n",
       " 4     33987a8ee5            NaN\n",
       " ...          ...            ...\n",
       " 3529  e5f0e6ef4b            NaN\n",
       " 3530  416863ce47            NaN\n",
       " 3531  6332da480c            NaN\n",
       " 3532  df1baec676            NaN\n",
       " 3533  469e15c5a8            NaN\n",
       " \n",
       " [3534 rows x 2 columns],\n",
       " 'test':           textID                                               text sentiment\n",
       " 0     f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n",
       " 1     96d74cb729   Shanghai is also really exciting (precisely -...  positive\n",
       " 2     eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n",
       " 3     01082688c6                                        happy bday!  positive\n",
       " 4     33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive\n",
       " ...          ...                                                ...       ...\n",
       " 3529  e5f0e6ef4b  its at 3 am, im very tired but i can`t sleep  ...  negative\n",
       " 3530  416863ce47  All alone in this old house again.  Thanks for...  positive\n",
       " 3531  6332da480c   I know what you mean. My little dog is sinkin...  negative\n",
       " 3532  df1baec676  _sutra what is your next youtube video gonna b...  positive\n",
       " 3533  469e15c5a8   http://twitpic.com/4woj2 - omgssh  ang cute n...  positive\n",
       " \n",
       " [3534 rows x 3 columns],\n",
       " 'train':            textID                                               text  \\\n",
       " 0      cb774db0d1                I`d have responded, if I were going   \n",
       " 1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       " 2      088c60f138                          my boss is bullying me...   \n",
       " 3      9642c003ef                     what interview! leave me alone   \n",
       " 4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       " ...           ...                                                ...   \n",
       " 27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       " 27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
       " 27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       " 27479  ed167662a5                         But it was worth it  ****.   \n",
       " 27480  6f7127d9d7     All this flirting going on - The ATG smiles...   \n",
       " \n",
       "                                            selected_text sentiment  \n",
       " 0                    I`d have responded, if I were going   neutral  \n",
       " 1                                               Sooo SAD  negative  \n",
       " 2                                            bullying me  negative  \n",
       " 3                                         leave me alone  negative  \n",
       " 4                                          Sons of ****,  negative  \n",
       " ...                                                  ...       ...  \n",
       " 27476                                             d lost  negative  \n",
       " 27477                                      , don`t force  negative  \n",
       " 27478                          Yay good for both of you.  positive  \n",
       " 27479                         But it was worth it  ****.  positive  \n",
       " 27480  All this flirting going on - The ATG smiles. Y...   neutral  \n",
       " \n",
       " [27481 rows x 4 columns]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "muslim-disposal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I`d have responded, if I were going'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "original-surgeon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I`d have responded, if I were going  Sooo SAD I will miss you here in San Diego!!! my boss is bully\n"
     ]
    }
   ],
   "source": [
    "train_corpus_all_in_one = ' '.join([tweet for tweet in documents['train']['text'].apply(lambda x : str(x))])\n",
    "\n",
    "print(train_corpus_all_in_one[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-perspective",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "killing-miracle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "returning-mistake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 438371\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "try:  # py3\n",
    "    all_tokens = [t for t in word_tokenize(train_corpus_all_in_one)]\n",
    "except UnicodeDecodeError:  # py27\n",
    "    all_tokens = [t for t in word_tokenize(train_corpus_all_in_one.decode('utf-8'))]\n",
    "\n",
    "print(\"Total number of tokens: {}\".format(len(all_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-basin",
   "metadata": {},
   "source": [
    "\n",
    "## Counting Words\n",
    "\n",
    "We start with a simple word count using `collections.Counter`\n",
    "\n",
    "We are interested in finding:\n",
    "\n",
    "  -  how many times a word occurs across the whole corpus (total number of occurrences)\n",
    "  -  in how many documents a word occurs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "crazy-newman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\t15296\n",
      ".\t13837\n",
      "`\t11614\n",
      "I\t11467\n",
      "to\t9885\n",
      ",\t8454\n",
      "the\t8431\n",
      "a\t6527\n",
      "i\t5135\n",
      "my\t4975\n",
      "*\t4953\n",
      "and\t4801\n",
      "you\t4794\n",
      "it\t4590\n",
      "?\t4266\n",
      "...\t3875\n",
      "is\t3741\n",
      "in\t3646\n",
      "for\t3597\n",
      "s\t3517\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "total_term_frequency = Counter(all_tokens)\n",
    "\n",
    "for word, freq in total_term_frequency.most_common(20):\n",
    "    print(\"{}\\t{}\".format(word, freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "miniature-sacrifice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\t9380\n",
      "`\t8804\n",
      "I\t8342\n",
      "!\t8129\n",
      "to\t7999\n",
      "the\t7014\n",
      ",\t6496\n",
      "a\t5753\n",
      "my\t4445\n",
      "and\t4328\n",
      "it\t4031\n",
      "you\t3973\n",
      "i\t3816\n",
      "is\t3478\n",
      "in\t3361\n",
      "for\t3336\n",
      "...\t3276\n",
      "s\t3182\n",
      "?\t2963\n",
      "of\t2870\n"
     ]
    }
   ],
   "source": [
    "document_frequency = Counter()\n",
    "\n",
    "for tweet in documents['train']['text'].apply(lambda x : str(x)):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    unique_tokens = set(tokens)\n",
    "    document_frequency.update(unique_tokens)\n",
    "\n",
    "for word, freq in document_frequency.most_common(20):\n",
    "    print(\"{}\\t{}\".format(word, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-degree",
   "metadata": {},
   "source": [
    "## Stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "crazy-plaza",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "179\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))\n",
    "print(len(stopwords.words('english')))\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "expensive-thursday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\t15296\n",
      ".\t13837\n",
      "`\t11614\n",
      "I\t11467\n",
      ",\t8454\n",
      "*\t4953\n",
      "?\t4266\n",
      "...\t3875\n",
      ":\t2255\n",
      "..\t1670\n",
      "day\t1613\n",
      "get\t1383\n",
      "like\t1302\n",
      "good\t1264\n",
      "http\t1243\n",
      "go\t1209\n",
      "-\t1207\n",
      "got\t1095\n",
      "work\t1090\n",
      "today\t1054\n"
     ]
    }
   ],
   "source": [
    "stop_list = stopwords.words('english') #+ list(string.punctuation)\n",
    "\n",
    "tokens_no_stop = [token for token in all_tokens\n",
    "                        if token not in stop_list]\n",
    "\n",
    "total_term_frequency_no_stop = Counter(tokens_no_stop)\n",
    "\n",
    "for word, freq in total_term_frequency_no_stop.most_common(20):\n",
    "    print(\"{}\\t{}\".format(word, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "broken-danish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294710"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dental-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_target(st):\n",
    "    st =  -1 if st==\"negative\" else 1 if st==\"positive\" else 0\n",
    "    return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "clear-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "vietnamese-curtis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i`d have responded, if i were going\t0\n",
      " sooo sad i will miss you here in san diego!!!\t-1\n",
      "my boss is bullying me...\t-1\n"
     ]
    }
   ],
   "source": [
    "train_data = documents['train']['text'].apply(lambda x : str(x).lower())\n",
    "train_labels = documents['train']['sentiment'].apply(lambda x : transform_target(x))\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"{}\\t{}\".format(train_data[i], train_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "metropolitan-burning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last session of the day  http://twitpic.com/67ezh\t0\n",
      " shanghai is also really exciting (precisely -- skyscrapers galore). good tweeps in china:  (sh)  (bj).\t1\n",
      "recession hit veronique branquinho, she has to quit her company, such a shame!\t-1\n"
     ]
    }
   ],
   "source": [
    "test_data = documents['test']['text'].apply(lambda x : str(x).lower())\n",
    "test_labels = documents['test']['sentiment'].apply(lambda x : transform_target(x))\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"{}\\t{}\".format(test_data[i], test_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fresh-mississippi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1 -1 ... -1  1  1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Representation of the data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorised_train_data = vectorizer.fit_transform(train_data)\n",
    "vectorised_test_data = vectorizer.transform(test_data)\n",
    "\n",
    "# Train the classifier given the training data\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(vectorised_train_data, train_labels)\n",
    "\n",
    "# Predict the labels for the test documents (not used for training)\n",
    "print(classifier.predict(vectorised_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "automotive-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "brief-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(test_labels, classifier.predict(vectorised_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "random-layer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.67      0.67      1001\n",
      "           0       0.64      0.66      0.65      1430\n",
      "           1       0.76      0.73      0.75      1103\n",
      "\n",
      "    accuracy                           0.68      3534\n",
      "   macro avg       0.69      0.69      0.69      3534\n",
      "weighted avg       0.69      0.68      0.68      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "immune-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "thick-journal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6876580952163184"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_labels, classifier.predict(vectorised_test_data), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "changing-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "widespread-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "rotary-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts = count_vect.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "secure-result",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26439"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dying-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "unauthorized-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.field]\n",
    "\n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[[self.field]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('colext', TextSelector('Text')),\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stop_words,\n",
    "                     min_df=.0025, max_df=0.25, ngram_range=(1,3))),\n",
    "            ('svd', TruncatedSVD(algorithm='randomized', n_components=300)), #for XGB\n",
    "        ])),\n",
    "        ('words', Pipeline([\n",
    "            ('wordext', NumberSelector('TotalWords')),\n",
    "            ('wscaler', StandardScaler()),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.1)),\n",
    "#    ('clf', RandomForestClassifier()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "     ('vect', TfidfVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
