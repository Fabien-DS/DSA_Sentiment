{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "preceding-religion",
   "metadata": {},
   "source": [
    "# Formation DSA - Fabien FAIVRE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-worst",
   "metadata": {},
   "source": [
    "**TD Antoine LY : analyse de sentiments**\n",
    "\n",
    "L'objectif est de vous faire approfondir une notion de machine learning aux travers d'une compétition Kaggle. Kaggle est aujourd'hui un site de compétition incontournable dans le monde du machine learning. Même si la plateforme n'est pas représentative des enjeux opérationnels, elle est néanmoins représentative des sujets d'attention de la communauté scientifique et demeure un bon outil d'apprentissage est de partage.\n",
    "\n",
    "\n",
    "Le projet consiste donc à utiliser le challenge [Tweet sentiment extraction](https://www.kaggle.com/c/tweet-sentiment-extraction/overview/description) à des fins académiques.\n",
    "\n",
    "\n",
    "### Les données\n",
    "\n",
    "Les données sont celles proposées par le challenge. Elles composent de deux fichiers:\n",
    "\n",
    "* `train.csv` ce document comportent les données à utiliser pour calibrer votre modèle. Il comporte toutes les colonnes\n",
    "* `test.csv` ce document n'est utilisé **que** pour évaluer la performance finale de votre modèle. En aucun cas il ne peut être utilisé pour fine-tuner ou calibrer votre modèle. Il simule les données qui ne sont normalement JAMAIS accessible sur Kaggle (ni dans la vraie vie). à considérer comme un nouvel échantillon.\n",
    "\n",
    "### Le challenge\n",
    "\n",
    "Prédire la colonne `sentiment` à partir de la colonne `text`.\n",
    "\n",
    "### La métrique d'évaluation\n",
    "\n",
    "On utilisera un score F1 à l'aide de la fonction implémentée dans `scikit-learn`\n",
    "\n",
    "    from sklearn.metrics import f1_score\n",
    "    y_true = [0, 1, 2, 0, 1, 2]\n",
    "    y_pred = [0, 2, 1, 0, 0, 1]\n",
    "    f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "\n",
    "### Labels à utiliser pour la colonne `sentiment`\n",
    "\n",
    "Vous devrez retraiter la colonne `sentiment` en utilisant les remplacements suivants:\n",
    "\n",
    "    \"neutral\"  ->  0\n",
    "    \"negative\" -> -1\n",
    "    \"positive\" ->  1\n",
    "\n",
    "### Informations pratiques sur le rendu et la notation.\n",
    "\n",
    "L'objectif est de se familiariser avec les techniques de text-mining à des fins de classification de sentiments d'un texte. La notation se décomposera en deux parties:\n",
    "\n",
    "#### Notation\n",
    "\n",
    "* Votre méthodologie et votre approche (12 points) : cette partie doit mettre en avant la motivation des différents retraitements que vous avez appliquez, votre effort de comprendre les implémentations des pacakges que vous aurez utilisés ainsi que le bon sens que leurs utilisations transcrit.\n",
    "* La performance finale et méthodologie (4 + 4 = 8 points) : cette notation sera relative au groupe. 3 tentatives d'algorithmes/preprocessing différents permettrons de garantir 4 points sur les 8. Les 3 premiers du classement (du groupe 2020) calculé à l'aide du score F1 sur la base de test atteigneront 4 points supplémentaires. Le reste du barême relatif au classement sera dégressif de façon linéaire par palier: les derniers obtenant 1 point minimum.\n",
    "\n",
    "#### Rendu\n",
    "\n",
    "Le rendu se fera sous la forme d'un court rapport (max 5 pages). Ce dernier peut se faire sous la forme d'un notebook (html ou pdf) ou d'un rapport traditionnel (word, pdf). Il doit mettre en avant la méthodologie employée, les difficultés rencontrées ainsi que les différents apprentissages.\n",
    "\n",
    "\n",
    "Le projet sera à rendre lors de la séance de **Juillet 2021** de restitution.\n",
    "\n",
    "### Language de programmation\n",
    "\n",
    "Il est fortement recommandé d'effectuer le projet en python, mais ceci n'est pas obligatoire.\n",
    "\n",
    "Bibliographie:\n",
    "\n",
    "https://www.scor.com/fr/articles-experts/accroitre-vitesse-et-precision-grace-lexploration-de-texte-et-au-traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-click",
   "metadata": {},
   "source": [
    "# Approche méthodologique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-joshua",
   "metadata": {},
   "source": [
    "L'approche suivie est double. Elle a consisté à :\n",
    "- prendre en main les approches de traitement du NLP en suivant la gradation historique (Wordvectors => TF-IdF => Word2Vec => DL avec notamment l'implémentation ` Twitter-roBERTa-base for Sentiment Analysis` sur [HuggingFace](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment).\n",
    "- s'approprier des techniques de MLOps au travers de l'utilisation et de l'adaptation d'un instanciation particulière de cookiecutter par [Manifold.ai](https://github.com/manifoldai/orbyter-cookiecutter). Ce template permet d'instancier deux environnement dockerisés (l'un pour l'environnement de développement et l'autre pour un serveur ML_Flow). L'ensemble des travaux a été versionné dans ce [repo github](https://github.com/Fabien-DS/DSA_Sentiment). L'objectif de cette difficulté compélmentaire était de tester en situation réel un ensemble d'outillages potentiellement intéressants pour MACIF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-idaho",
   "metadata": {},
   "source": [
    "## 0) Création des prérequis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-smoke",
   "metadata": {},
   "source": [
    "Cette section sert à mettre en place l'environnement de travail\n",
    "- charger les packages nécessaires au projet\n",
    "- permettre de refactoriser le code du projet sous forme de package\n",
    "- mettre un place un tracking d'experiment avec ML_Flow pour suivre les itérations du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-liberia",
   "metadata": {},
   "source": [
    "### Import des packages utilisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "floral-metadata",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "import spacy \n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-biodiversity",
   "metadata": {},
   "source": [
    "### Utilisation du code du projet packagé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dietary-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cette cellule permet d'appeler la version packagée du projet et d'en assurer le reload avant appel des fonctions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-genetics",
   "metadata": {},
   "source": [
    "### Configuration de l'experiment MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prompt-accident",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/experiments'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.tracking.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "controlling-driving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'DSA_sentiment' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "exp_name=\"DSA_sentiment\"\n",
    "mlflow.set_experiment(exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-patient",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "public-cartoon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/raw/sample_submission.csv',\n",
       " '../data/raw/test.csv',\n",
       " '../data/raw/train.csv']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = os.path.join('..', 'data', 'raw')\n",
    "all_raw_files = [os.path.join(data_folder, fname)\n",
    "                    for fname in os.listdir(data_folder)]\n",
    "all_raw_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "acute-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {}\n",
    "for text_fname in all_raw_files:\n",
    "    bname = os.path.basename(text_fname).split('.')[0]\n",
    "    documents[bname] = pd.read_csv(text_fname, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "genetic-blackjack",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(documents['train'])\n",
    "test = pd.DataFrame(documents['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "hollywood-message",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "systematic-constitution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27475</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Y...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "27475  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       "27476  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
       "27477  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       "27478  ed167662a5                         But it was worth it  ****.   \n",
       "27479  6f7127d9d7     All this flirting going on - The ATG smiles...   \n",
       "\n",
       "                                           selected_text sentiment  \n",
       "27475                                             d lost  negative  \n",
       "27476                                      , don`t force  negative  \n",
       "27477                          Yay good for both of you.  positive  \n",
       "27478                         But it was worth it  ****.  positive  \n",
       "27479  All this flirting going on - The ATG smiles. Y...   neutral  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "unnecessary-soldier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         27481 non-null  object\n",
      " 1   text           27480 non-null  object\n",
      " 2   selected_text  27480 non-null  object\n",
      " 3   sentiment      27481 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 858.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3534 entries, 0 to 3533\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   textID     3534 non-null   object\n",
      " 1   text       3534 non-null   object\n",
      " 2   sentiment  3534 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 83.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acquired-referral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : \n",
      " textID           0\n",
      "text             1\n",
      "selected_text    1\n",
      "sentiment        0\n",
      "dtype: int64 \n",
      " test : \n",
      " textID       0\n",
      "text         0\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'train : \\n',\n",
    "    train.isna().sum(), \n",
    "    '\\n',\n",
    "    'test : \\n',\n",
    "    test.isna().sum()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-wells",
   "metadata": {},
   "source": [
    "Il n'est pas possible de faire de l'imputation comme avec des champs numérique. Il convient donc de supprimer les entrées vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adequate-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)\n",
    "train = train.reset_index(drop=True)\n",
    "test.dropna(inplace=True)\n",
    "test = test.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-sacramento",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "mental-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.field]\n",
    "\n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[[self.field]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "beginning-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    porter_stemmer=nltk.PorterStemmer()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "processed-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_target(st):\n",
    "    st =  -1 if st==\"negative\" else 1 if st==\"positive\" else 0\n",
    "    return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "posted-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('colext', TextSelector('text')), #Sélection de la colonne à transformer (corpus)\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stopwords.words('english'), #Sélection de la colonne à transformer (corpus)\n",
    "                     min_df=.0025, max_df=0.25, ngram_range=(1,3))),\n",
    "            ('svd', TruncatedSVD(algorithm='randomized', n_components=300)), #for XGB : linear dimensionality reduction by means of truncated singular value decomposition (SVD)\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.1)),\n",
    "#    ('clf', RandomForestClassifier()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "acting-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(train['text'].apply(lambda x : str(x).lower()), columns=['text'])\n",
    "y_train=train['sentiment'].apply(lambda x : transform_target(x))\n",
    "X_test = pd.DataFrame(test['text'].apply(lambda x : str(x).lower()), columns=['text'])\n",
    "y_test=test['sentiment'].apply(lambda x : transform_target(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "permanent-alignment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1   -1\n",
       "2   -1\n",
       "3   -1\n",
       "4   -1\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "thousand-study",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i`d have responded, if i were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sooo sad i will miss you here in san diego!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sons of ****, why couldn`t they put them on t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                i`d have responded, if i were going\n",
       "1      sooo sad i will miss you here in san diego!!!\n",
       "2                          my boss is bullying me...\n",
       "3                     what interview! leave me alone\n",
       "4   sons of ****, why couldn`t they put them on t..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "superb-configuration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:42:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('text',\n",
       "                                                 Pipeline(steps=[('colext',\n",
       "                                                                  TextSelector(field='text')),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfVectorizer(max_df=0.25,\n",
       "                                                                                  min_df=0.0025,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               3),\n",
       "                                                                                  stop_words=['i',\n",
       "                                                                                              'me',\n",
       "                                                                                              'my',\n",
       "                                                                                              'myself',\n",
       "                                                                                              'we',\n",
       "                                                                                              'our',\n",
       "                                                                                              'ours',\n",
       "                                                                                              'ourselves',\n",
       "                                                                                              'you',\n",
       "                                                                                              \"you're\",\n",
       "                                                                                              \"you've\",\n",
       "                                                                                              \"you'll\",\n",
       "                                                                                              \"you'd\",\n",
       "                                                                                              'your',\n",
       "                                                                                              'yours',\n",
       "                                                                                              'yourself',\n",
       "                                                                                              'yourselves',\n",
       "                                                                                              'he',\n",
       "                                                                                              'him',\n",
       "                                                                                              'his...\n",
       "                               importance_type='gain',\n",
       "                               interaction_constraints='', learning_rate=0.1,\n",
       "                               max_delta_step=0, max_depth=3,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=300,\n",
       "                               n_jobs=8, num_parallel_tree=1,\n",
       "                               objective='multi:softprob', random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "                               subsample=1, tree_method='exact',\n",
       "                               validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "leading-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "vertical-north",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6569194215147925"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_test_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "continent-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('colext', TextSelector('text')), #Sélection de la colonne à transformer (corpus)\n",
    "            ('tfidf', TfidfVectorizer())\n",
    "        ]))\n",
    "    ])),\n",
    "    ('clf', LinearSVC()),\n",
    "#    ('clf', RandomForestClassifier()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "median-citizenship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('text',\n",
       "                                                 Pipeline(steps=[('colext',\n",
       "                                                                  TextSelector(field='text')),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfVectorizer())]))])),\n",
       "                ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "vertical-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred2 = classifier2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "behavioral-cooperation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6873879020805563"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_test_pred2, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "plain-module",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6902256127441538"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('colext', TextSelector('text')), #Sélection de la colonne à transformer (corpus)\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"classifier\", RandomForestClassifier(n_jobs=-1)),\n",
    "    ]\n",
    ")\n",
    "bow_pipeline.fit(X_train, y_train)\n",
    "y_pred = bow_pipeline.predict(X_test)\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "normal-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fleet-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")  # this model will give you 300D\n",
    "#nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "incorrect-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.dim = 300\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Doc.vector defaults to an average of the token vectors.\n",
    "        # https://spacy.io/api/doc#vector\n",
    "        return [self.nlp(text).vector for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "supreme-aerospace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6043214555569574"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('colext', TextSelector('text')), #Sélection de la colonne à transformer (corpus)\n",
    "        (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
    "        (\"reduce_dim\", TruncatedSVD(50)),\n",
    "        (\"classifier\", RandomForestClassifier(n_jobs=-1)),\n",
    "    ]\n",
    ")\n",
    "embeddings_pipeline.fit(X_train, y_train)\n",
    "y_pred = embeddings_pipeline.predict(X_test)\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "gothic-subscriber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'colext', 'mean_embeddings', 'reduce_dim', 'classifier', 'colext__field', 'mean_embeddings__nlp', 'reduce_dim__algorithm', 'reduce_dim__n_components', 'reduce_dim__n_iter', 'reduce_dim__random_state', 'reduce_dim__tol', 'classifier__bootstrap', 'classifier__ccp_alpha', 'classifier__class_weight', 'classifier__criterion', 'classifier__max_depth', 'classifier__max_features', 'classifier__max_leaf_nodes', 'classifier__max_samples', 'classifier__min_impurity_decrease', 'classifier__min_impurity_split', 'classifier__min_samples_leaf', 'classifier__min_samples_split', 'classifier__min_weight_fraction_leaf', 'classifier__n_estimators', 'classifier__n_jobs', 'classifier__oob_score', 'classifier__random_state', 'classifier__verbose', 'classifier__warm_start'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "reliable-garden",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'colext', 'tfidf', 'classifier', 'colext__field', 'tfidf__analyzer', 'tfidf__binary', 'tfidf__decode_error', 'tfidf__dtype', 'tfidf__encoding', 'tfidf__input', 'tfidf__lowercase', 'tfidf__max_df', 'tfidf__max_features', 'tfidf__min_df', 'tfidf__ngram_range', 'tfidf__norm', 'tfidf__preprocessor', 'tfidf__smooth_idf', 'tfidf__stop_words', 'tfidf__strip_accents', 'tfidf__sublinear_tf', 'tfidf__token_pattern', 'tfidf__tokenizer', 'tfidf__use_idf', 'tfidf__vocabulary', 'classifier__bootstrap', 'classifier__ccp_alpha', 'classifier__class_weight', 'classifier__criterion', 'classifier__max_depth', 'classifier__max_features', 'classifier__max_leaf_nodes', 'classifier__max_samples', 'classifier__min_impurity_decrease', 'classifier__min_impurity_split', 'classifier__min_samples_leaf', 'classifier__min_samples_split', 'classifier__min_weight_fraction_leaf', 'classifier__n_estimators', 'classifier__n_jobs', 'classifier__oob_score', 'classifier__random_state', 'classifier__verbose', 'classifier__warm_start'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "pediatric-training",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6969276370587574"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV# the keys can be accessed with final_pipeline.get_params().keys()\n",
    "params = {\n",
    "    \"tfidf__use_idf\": [True, False],\n",
    "    \"tfidf__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"classifier__bootstrap\": [True, False],\n",
    "    \"classifier__class_weight\": [\"balanced\", None],\n",
    "    \"classifier__n_estimators\": [100, 300, 500, 800, 1200],\n",
    "    \"classifier__max_depth\": [5, 8, 15, 25, 30],\n",
    "    \"classifier__min_samples_split\": [2, 5, 10, 15, 100],\n",
    "    \"classifier__min_samples_leaf\": [1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(bow_pipeline, params)\n",
    "search.fit(X_train, y_train)\n",
    "y_pred = search.predict(X_test)\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "answering-capacity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__use_idf': False,\n",
       " 'tfidf__ngram_range': (1, 1),\n",
       " 'classifier__n_estimators': 1200,\n",
       " 'classifier__min_samples_split': 10,\n",
       " 'classifier__min_samples_leaf': 2,\n",
       " 'classifier__max_depth': 25,\n",
       " 'classifier__class_weight': 'balanced',\n",
       " 'classifier__bootstrap': True}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "norwegian-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "sorted-instrumentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.69      0.68      1001\n",
      "           0       0.66      0.68      0.67      1430\n",
      "           1       0.76      0.72      0.74      1103\n",
      "\n",
      "    accuracy                           0.69      3534\n",
      "   macro avg       0.70      0.70      0.70      3534\n",
      "weighted avg       0.70      0.69      0.69      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cr = classification_report(y_test, y_pred)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-anger",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
