{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vietnamese-thought",
   "metadata": {},
   "source": [
    "# Enseignements et pistes d'amélioration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-nightlife",
   "metadata": {},
   "source": [
    "Ce projet a été une constant source d'étonnement.\n",
    "\n",
    "Le fait de disposer de 3 classes à prédire a été un élément complexifiant par rapport au cas binaire (pas de courbe ROC générale). On devient beaucoup plus dépendant des chiffres.\n",
    "\n",
    "Après la découverte de [twitter-roberta-base-sentiment](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment), je pensais que le sujet serait plié. Après tout ce modèle a été entrainé explicitement pour ce cas (58 millions de tweets, en anglais et optimisé pour l'analyse de sentiments). Je pensais voir le f1 macro s'envoler, ce qui n'a pas été le cas. Le modèle a bien aidé, mais le gain est resté modeste (6 points de f1 macro par rapport aux approches fréquentistes classiques).\n",
    "\n",
    "Au final en analysant les fausses prédictions, on réalise que la labelisation de plusieurs tweets laisse songeur.\n",
    "Ceci met en lumière le fait que l'appréciation de la tonalité n'est pas toujours évidente et que des erreurs humaines peuvent en plus se glisser.\n",
    "Si ce phénomène existe déjà pour les catégories extrèmes (`positif` et `négatif`) on imagine la sensibilité pour la classe générique `neutre`...\n",
    "\n",
    "Par ailleurs rien n'indique que la stratégie de labellisation utilisé dans ce cas corresponde à celle utilisée pour le pré entrainement de roBERTa tweet.\n",
    "\n",
    "Deux pistes auraient pu être explorées pour améliorer la performance :\n",
    "- véritablement réentrainer la dernière couche de BERT sur le jeu de donénes pour apprendre la logique de classification\n",
    "- potentiellement modéliser le sujet discuté dans les twwets et le rajouter comme feature. On avait en effet vu que les tweets positifs par exemple se rapportaient principalement à la fête des mère et au `star wars day`\n",
    "\n",
    "Enfin ce sujet a été l'occasion de se frotter à plusieurs difficultés techniques liées principalement :\n",
    "- à l'utilisation de ressources GPU depuis docker\n",
    "- à l'utilisation des GPU pour XGBoost (non pris en compte par défaut)\n",
    "- aux pipelines sklearn, pratiques mais pas toujousr compatibles avec les packages (ex SHAP) et nécessitant souvent des créations de classes ad-hoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
