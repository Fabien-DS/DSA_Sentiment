#!/usr/bin/env python
# coding: utf-8

# -------------------------------------------------------------------
# **TD DSA 2021 de Antoine Ly   -   rapport de Fabien Faivre**
# -------------------------     -------------------------------------

# # Mod√©lisation

# ## Setup

# In[1]:


get_ipython().system('pip install textblob')


# In[2]:


get_ipython().system('pip install emot')


# In[3]:


get_ipython().system('pip install wordcloud')


# In[34]:


get_ipython().system('pip install lime')


# In[1]:


#Temps et fichiers
import os
import warnings
import time
from datetime import timedelta

#Manipulation de donn√©es
import pandas as pd
import numpy as np


# Text
from collections import Counter
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
from nltk.corpus import stopwords
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
from nltk.util import ngrams

from textblob import TextBlob
import string
import re
import spacy 
from emot.emo_unicode import UNICODE_EMO, EMOTICONS

#Mod√©lisation
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.base import BaseEstimator, TransformerMixin

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import TruncatedSVD

from sklearn.svm import LinearSVC
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

from xgboost import XGBClassifier


#Evaluation
from sklearn.metrics import f1_score, confusion_matrix, classification_report, precision_score, recall_score


#Visualisation
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from wordcloud import WordCloud

#Tracking d'exp√©rience
import mlflow
import mlflow.sklearn


# In[284]:


import shap

shap.initjs()


# In[5]:


#Cellule strictement technique qui permet de sauver les exigences pour recr√©er au besoin l'image docker du projet
get_ipython().system('pip freeze > /mnt/docker/requirements.txt')


# ### Utilisation du package

# Durent ce projet, certaines parties du code ont √©t√© re packag√©es dans un package propre au projet afin de factliter la lecture du core et permettre la r√©utilisabilit√© des d√©veloppements

# In[2]:


#Cette cellule permet d'appeler la version packag√©e du projet et d'en assurer le reload avant appel des fonctions
get_ipython().run_line_magic('load_ext', 'autoreload')
get_ipython().run_line_magic('autoreload', '2')


# In[3]:


from dsa_sentiment.scripts.make_dataset import load_data
from dsa_sentiment.scripts.evaluate import eval_metrics
from dsa_sentiment.scripts.make_dataset import Preprocess_StrLower, Preprocess_transform_target


# ### Configuration de l'experiment MLFlow

# [MLFlow](https://mlflow.org/) sera utilis√© comme outil de suivi et de stockage des exp√©rimentatiosn r√©alis√©es

# In[7]:


mlflow.tracking.get_tracking_uri()


# ### Chargement des donn√©es

# In[4]:


# On Importe les donn√©es

#df
df_train=pd.read_parquet('/mnt/data/interim/df_train.gzip')
df_val=pd.read_parquet('/mnt/data/interim/df_val.gzip')
df_test=pd.read_parquet('/mnt/data/interim/df_test.gzip')

#X
X_train=pd.read_parquet('/mnt/data/interim/X_train.gzip')
X_val=pd.read_parquet('/mnt/data/interim/X_val.gzip')
X_test=pd.read_parquet('/mnt/data/interim/X_test.gzip')

X_train_prepro=pd.read_parquet('/mnt/data/interim/X_train_prepro.gzip')
X_val_prepro=pd.read_parquet('/mnt/data/interim/X_val_prepro.gzip')
X_test_prepro=pd.read_parquet('/mnt/data/interim/X_test_prepro.gzip')

#y
y_train=pd.read_parquet('/mnt/data/interim/y_train.gzip')
y_val=pd.read_parquet('/mnt/data/interim/y_val.gzip')
y_test=pd.read_parquet('/mnt/data/interim/y_test.gzip')


# In[200]:


pd.options.display.max_colwidth=300


# ## Mod√©lisation

# ### Cr√©ation du code g√©n√©rique 

# On commence par d√©finir une fonction g√©n√©rique qui sera en capacit√© d'ajuster, optimiser et logger dans MLFlow les r√©sultats de pipelines qui seront produits pour chaque essai

# Le mode de fonctionnement souhait√© consiste √† 
# 
# 1- d√©finir un pipeline au sens de sklearn
# 
# 2- utiliser une fonction g√©n√©rique pour ajuster le pipeline (√©ventuellement en optimisant les param√®tres) et en stocker le r√©sultat dans MLFlow

# #### Pr√©alables : cr√©ation des fonctions de r√©sultat souhait√©es

# La premi√®re √©tape consiste √† construire une fonction g√©n√©rique qui calculera **les scores du pipeline que nous souhaitons suivre**.
# Dans le cas pr√©sent comme l'exercice de classification est multiclasse, nous sommes int√©ress√©s par les `f1`, `precision` et `recall` calcul√©s avec l'option `macro` qui r√©alise une moyenne des r√©sultats obtenus par classe.

# In[5]:


def score_estimator(
    estimator, X_train, X_test, df_train, df_test, target_col
):
    
    """
    Evalue un pipeline sur le jeu de train et test avec plusieurs m√©triques
    
    Ici les m√©triques utilis√©es sont :
    - f1 macro
    - precision macro
    - recall macro
    
    INPUTS :
        - estimator : un pipeline
        - X_train, X_test, df_train, df_test : les DataFrames contenant les jeux de donn√©es et test
        - target_col : le nom de la colonne cible dans les df
        
    OUTPUTS :
        - un DataFrame avec les m√©triques calcul√©es sur les jeux de train et test fournis
    
    
    """

    metrics = [
        ("f1_macro", f1_score),   
        ("precision_macro", precision_score),
        ("recall_macro", recall_score),
        
    ]
    
    res = []
    for subset_label, X, df in [
        ("train", X_train, df_train),
        ("test", X_test, df_test),
    ]:
        y = df[target_col]
        y_pred = estimator.predict(X)
        for score_label, metric in metrics:
            score = metric(y, y_pred, average='macro')
            res.append(
                {"subset": subset_label, "metric": score_label, "score": score}
            )

    res = (
        pd.DataFrame(res)
        .set_index(["metric", "subset"])
        .score.unstack(-1)
        .round(4)
        .loc[:, ['train', 'test']]
    )
    return res


# Pour pouvoir stocker les scores dans MLFlow, on les convertit en dictionnaires

# In[6]:


def scores_to_dict(score_df):
    d = score_df['train'].to_dict()
    d1 = dict(zip([x+'_train_' for x in  list(d.keys())], list(d.values())))
    d = score_df['test'].to_dict()
    d2 = dict(zip([x+'_test' for x in  list(d.keys())], list(d.values())))
    d1.update(d2)
    return d1


# Cr√©ation d'une fonction affichant une matrice de confusion

# In[7]:


def plot_cm(y_test, y_pred, target_names=[-1, 0, 1], 
            figsize=(5,3)):
    """Create a labelled confusion matrix plot."""
    cm = confusion_matrix(y_test, y_pred)
    fig, ax = plt.subplots(figsize=figsize)
    sns.heatmap(cm, annot=True, fmt='g', cmap='BuGn', cbar=False, 
                ax=ax)
    ax.set_title('Confusion matrix')
    ax.set_xlabel('Predicted')
    ax.set_xticklabels(target_names)
    ax.set_ylabel('Actual')
    ax.set_yticklabels(target_names, 
                       fontdict={'verticalalignment': 'center'});


# #### Cr√©ation de la fonction d'entra√Ænement g√©n√©rique

# La fonction suivante est celle qui sera syst√©matiquement app√©l√©e pour entra√Æner les pipelines

# :::{tip}
# L'√©valuation fianle des mod√®les se faisant sur base de f1-macro dans le TD, c'est la m√©trique que nosu avons retenue pour la partie optimisation de la fonction g√©n√©rique
# :::

# In[8]:


def trainPipelineMlFlow(mlf_XP, 
                        xp_name_iter, 
                        pipeline, 
                        X_train, y_train, X_test, y_test, 
                        target_col='sentiment', 
                        fixed_params={}, 
                        use_opti=False, iterable_params={}, n_iter=20):
    """
    Fonction g√©n√©rique permettant d'entrainer et d'optimiser un pipeline sklearn
    Les param√®tres et r√©sultats sont stock√©s dans MLFlow
    
    INPUTS:
        - mlf_XP : nom de l'experiment √† cr√©er dans MLFlow
        - xp_name_iter : nom du run cr√©√© dans l'experiment de MLFlow
        - pipeline : un pipeline au sens ed sklearn
        - X_train, y_train, X_test, y_test : des dataframes contenant les jeux d'entrainement et de test
        - target_col : le nom de la colonne du DataFrame y qui constitue la cible
        - fixed_params : un dictionnaire contenant les param√®tres fixes dont l'utilisateur souhaite fixer la valeur dans le pipeline
        - use_opti : boolean, est-ce qu'une optimisation est recherch√©e. Si oui, utilisera RandomizedSearchCV
        - iterable_params : un dictionnaire contenant les nom des param√®tres cibl√©s du pipeline et des listes contenant les valeusr possibles
        - n_iter : le nombre d'it√©rations maximales √† r√©aliser par RandomizedSearchCV
    
    FONCTIONNEMENT:
        stocke dans MLFlow :
        - le pipeline entrain√©
        - les principaux param√®tres correspondant aux param√®tres fixes et aux √©ventuels param√®tres optimaux apr√®s RandomizedSearchCV
        - les scores (scalaires) calcul√©s par la fonction score_estimator
        - le temps d'ex√©cution
        
        imprime :
        - le nom de l'experiment
        - le pipeline entra√Æn√©
        - les param√®tres principaux (cf FONCTIONNEMENT)
        - la matrice de confusion du pipeline sur le jeu de test fourni en entr√©e
    
    OUTPUTS:
        - le pipeline entra√Æn√©
    
       
    """
  
    mlflow.set_experiment(mlf_XP)

    with mlflow.start_run(run_name=xp_name_iter):
        
        start_time = time.monotonic()  
        
        warnings.filterwarnings("ignore")
        
        # fit pipeline
        pipeline.set_params(**fixed_params)
        if not use_opti:
            search = pipeline
        else:
            search = RandomizedSearchCV(estimator = pipeline, 
                                        param_distributions = iterable_params, 
                                        n_jobs = -1, 
                                        cv = 5, 
                                        scoring = 'f1_macro', 
                                        n_iter = n_iter,
                                        random_state = 42
                                       )
        
        search.fit(X_train, y_train[target_col])
                
        # get params
        params_to_log = fixed_params #select initial params
        if use_opti:
            params_to_log.update(search.best_params_) #update for optimal solution
        mlflow.log_params(params_to_log)
        
        # Evaluate metrics
        y_pred=search.predict(X_test)
        score = score_estimator(estimator=search, 
                                         X_train=X_train, 
                                         X_test=X_test, 
                                         df_train=y_train, 
                                         df_test=y_test, 
                                         target_col=target_col
                                )
        
        # Print out metrics
        print('XP :', xp_name_iter, '\n')
        print('pipeline : \n', search, '\n')
        print("params: \n", params_to_log, '\n')
        print('scores : \n', score, '\n')
        print("Test confusion matrix: \n")
        plot_cm(y_test, search.predict(X_test))
        
        
        #r Report to MlFlow
        mlflow.log_metrics(scores_to_dict(score))
        mlflow.sklearn.log_model(pipeline, xp_name_iter)
        
        end_time = time.monotonic()
        elapsed_time = timedelta(seconds=end_time - start_time)
        print('elapsed time :', elapsed_time)
        mlflow.set_tag(key="elapsed_time", value=elapsed_time)   
        
        
        
    return search;
        


# #### Utilitaires : pour faciliter l'utilisation des pipelines

# Si les pipelines permettent un traitement souple et homog√®ne entre les jeux de donn√©es, leur manipulation n'est pas √©vidente.
# Notamment, le libell√© des param√®tres peut vide devenir d√©licat et difficilement lisible avec une combinaison de nom d'√©tape et du nom du param√®tre dans l'√©tape du pipeline.
# La fonction suivante permet de rechercher tous les param√®tres d'un pipeline qui contiennent une chaine de caract√®re sp√©cifique.

# In[9]:


def target_params(pipe, dict_keyval):
    """
    Cr√©e un dictionnaire constitu√© de tous les param√®tres incluant 'pattern' d'un pipe et leur assigne une valeur unique
    """
    
    res={}
    for key in list(dict_keyval.keys()):
    
        target = "[a-zA-Z\_]+__" + key

        rs = re.findall(target, ' '.join(list(pipe.get_params().keys())))
        rs=dict.fromkeys(rs, dict_keyval[key])
        res.update(rs)
    return res


# #### Utilitaires : Adaptation des pipelines

# La cellule suivante permet de cr√©er des √©tapes de s√©lection de colonnes dans les Data Frame en entr√©e

# In[151]:


from sklearn.base import BaseEstimator, TransformerMixin

class TextSelector(BaseEstimator, TransformerMixin):
    def __init__(self, field):
        self.field = field
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        if isinstance(X,(list,pd.core.series.Series,np.ndarray)): #permet d'avoir une structure souple si l'input n'est pas un DataFrame. Permet notamment d'utiliser LIME
            return X
        else:
            return X[self.field]


# #### Utilitaires : Visualisation

# In[159]:


def plotWc(text, stopwords=None, title=''):
    wc = WordCloud(
            stopwords=stopwords,
            width=800,
            height=400,
            max_words=1000,
            random_state=44,
            background_color="white",
            collocations=False
    ).generate(text)
    
    plt.figure(figsize = (10,10))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title(title)
    plt.show()


# ### Approche initiale

# On commence par construire un mod√®le simple qui nous servira de mod√®le de base que nous chercherons √† am√©liorer.

# :::{warning}
# 
# Dans cette premi√®re √©tape, nous travaillerons sur le jeu `train` que nous avon d√©coup√© et √©valuerons ses performances sur le jeu `val`.
# 
# Seuls les principaux mod√®les seront r√©entrain√©s sur `{ train + val }` avant d'√™tre √©valu√©s sur le v√©ritable jeu `test`
# 
# :::

# On suit les mod√®les dans un DataFrame r√©sultats

# In[219]:


r√©sultats = pd.DataFrame(columns=['mod√®le', 'f1_macro_val'])
r√©sultats


# #### Bag of Words avec Random Forest

# Dans cette exp√©rimentation, nous cr√©ons un mod√®le simple :
# 
# ![BoW_RF](images/Pipeline_BoW_RF.png)

# La classe `TfidfVectorizer` de sklearn permet d'appliquer ou non le traitement TF-IDF et dans ce dernier cas de travailler directement avec un Bag of Words 

# In[153]:


tfidf_RF_pipeline = Pipeline(
    steps=[
        ('coltext', TextSelector('text')),
        ("tfidf", TfidfVectorizer()),
        ("classifier", RandomForestClassifier(n_jobs=-1)),
    ]
)


# D√©j√† dans cet exemple simple, le nombre de param√®tres est important et leur nom vite complexe :

# In[154]:


list(tfidf_RF_pipeline.get_params().keys())


# En premi√®re intention on ajuste le pipeline sur le jeu d'entra√Ænement avant les √©tapes de preprocessing r√©alis√©es lors de l'EDA

# In[304]:


pipe = tfidf_RF_pipeline


base_tfidf_RF_= trainPipelineMlFlow(
                    mlf_XP = "Rapport",
                    xp_name_iter = "base_tfidf_RF", 
                    pipeline = pipe, 
                    X_train = X_train, y_train = y_train, X_test = X_val, y_test = y_val,
                    target_col = 'sentiment',
                    fixed_params = target_params( pipe , {'n_jobs':-1, 'random_state':42})
                    );


# Le mod√®le de base produit un f1 macro de **67,81%** sur le jeu de validation avec le param√©trage par d√©faut de sklearn.
# On observe le tr√®s fort f1 macro sur le jeu d'entra√Ænement qui indique un fort surapprentissage.
# L'int√©r√™t de ce pipeline est d'√™tre tr√®s rapide √† l'entra√Ænement (√† peine plus de 3 secondes ici)

# On voit bien que la difficult√© viendra de la classe neutre qui peut facilement √™tre confondue avec les classes n√©gatives ou positives
# 
# Par contre, il est plus surprenant de voir des tweets positifs class√©s en n√©gatifs et inversement.
# 
# La suite investigue ce ph√©nom√®ne

# In[305]:


pipe = base_TfIdf_RF_

y_val_pred = pipe.predict(X_val)

exemples_realNeg_predPos = X_val[(y_val['sentiment']==-1) & (y_val_pred==1)]
exemples_realPos_predNeg = X_val[(y_val['sentiment']==1) & (y_val_pred==-1)]


# ##### Analyse des tweets n√©gatifs class√© positifs

# In[306]:


exemples_realNeg_predPos


# In[307]:


plotWc(" ".join(exemples_realNeg_predPos['text']), stopwords=stopwords.words('english'), title = "Wordcloud des tweets n√©gatifs pr√©dits positifs")


# On voit les limites des approches de type Bag of Words : le mod√®le est tromp√© par des mots √† connotation positive sans en comprendre l'encha√Ænement

# ##### Analyse des tweets positifs class√© n√©gatifs

# In[308]:


exemples_realPos_predNeg


# On observe deux ph√©nom√®nes contraires :
# - vraisemblablement des probl√®mes de labelisation (ex 22281 class√© positifs...)
# - des tweets manifestements positifs sans pi√®ges et pourtant class√©s n√©gatifs (25367)

# In[299]:


plotWc(" ".join(exemples_realPos_predNeg['text']), stopwords=stopwords.words('english'), title = "Wordcloud des tweets positifs pr√©dits n√©gatifs")


# pour essayer de comprendre ce qu'il s'est pass√© sur l'instance 22238, on peut essayer d'analyser le r√©sultat √† partir de Lime :

# In[309]:


exemples_realPos_predNeg['text'][25367]


# In[310]:


from lime import lime_text
from lime.lime_text import LimeTextExplainer
explainer = LimeTextExplainer(class_names=['negative', 'neutral', 'positive'])

exp = explainer.explain_instance(exemples_realPos_predNeg['text'][25367], base_TfIdf_RF_.predict_proba, top_labels=1)
exp.show_in_notebook(text=True)


# Le r√©sultat est tr√®s surprenant. On peut imaginer que really est utilis√© plus frequemment dans des tweets n√©gatifs

# In[311]:


def list_words_with(text_series, search='', nb=30):
    '''
    Cette fonction permet de lister les mots dans un string qui contiennent une certaine cha√Æne de caract√®res
    
    inputs :
        - text_series : un pd.Series contennat les cha√Ænes de caract√®res
        - search : la s√©quence √† rechercher
        - nb : ressortir les nb occurences les plus fr√©quentes
    
    output :
        - une liste de tuples contenant 
            + le mot contenant la s√©quence recherch√©e
            + le nombre d'occurence dans text_series
    
    '''
    
    
    #searchPattern   = f"\w*{search}\w*"
    searchPattern   = f"\w*{search}\w* "
    
    cnt = Counter()
    
    for tweet in text_series:
        # Replace all URls with 'URL'
        tweet = re.findall(searchPattern,tweet)
        for word in tweet:
            cnt[word] += 1
    return cnt.most_common(nb)
    


# In[312]:


list_words_with(X_train['text'][y_train['sentiment']==-1], 'really')


# In[313]:


list_words_with(X_train['text'][y_train['sentiment']==1], 'really')


# Ce qui est bien le cas

# In[220]:


item = pd.DataFrame([['base_TfIdf_RF_', f1_score(y_val, base_TfIdf_RF_.predict(X_val),average='macro')]], columns=['mod√®le', 'f1_macro_val'])

item


# In[221]:


r√©sultats = r√©sultats.append(item)
r√©sultats


# #### variante preprocessing

# On peut juger de l'int√©r√™t des pr√©traitements que nous avons r√©alis√©s en changeant le jeu d'entr√©e :
# 
# ![RF_prepro](images/Pipeline_BoW_RF_prepro.png)

# In[216]:


pipe = tfidf_RF_pipeline


base_TfIdf_RF_prepro_= trainPipelineMlFlow(
                            mlf_XP = "Rapport",
                            xp_name_iter = "base_TfIdf_RF_prepro", 
                            pipeline = pipe, 
                            X_train = X_train_prepro, y_train = y_train, X_test = X_val_prepro, y_test = y_val,
                            target_col = 'sentiment',
                            fixed_params = target_params( pipe , {'n_jobs':-1, 'random_state':42})
                        );


# :::{admonition} apport du preprocessing
# On observe tout de suite l'apport des retraitemenst effectu√©s √† l'√©tape EDA : le mod√®le est pass√© √† une performance de **70,79%** sur le jeu de validation sans autres modifications
# :::

# In[222]:


item = pd.DataFrame([['base_TfIdf_RF_prepro_', f1_score(y_val, base_TfIdf_RF_prepro_.predict(X_val_prepro),average='macro')]], columns=['mod√®le', 'f1_macro_val'])

item


# In[223]:


r√©sultats = r√©sultats.append(item)
r√©sultats


# #### variante optimis√©e

# Une autre variante consiste √† essayer d'ajuster les hyper param√®tres du pipeline dans l'espoire de gagner en performance.
# On reste sur le jeu de donn√©es pr√©trait√©

# In[225]:


pipe = tfidf_RF_pipeline


params = target_params( pipe , {
                                "use_idf": [True, False],
                                "ngram_range": [(1, 1), (1, 2), (1,3)],
                                "bootstrap": [True, False],
                                "class_weight": ["balanced", None],
                                "n_estimators": [100, 300, 500],
                                })


base_TfIdf_RF_prepro_opti_= trainPipelineMlFlow(
                                            mlf_XP = "Rapport",
                                            xp_name_iter = "base_TfIdf_RF_prepro_opti", 
                                            pipeline = pipe, 
                                            X_train = X_train_prepro, y_train = y_train, X_test = X_val_prepro, y_test = y_val,
                                            target_col = 'sentiment',
                                            fixed_params = target_params( pipe , {'n_jobs':-1, 'random_state':42}),
                                            use_opti = True,
                                            iterable_params = params,
                                            n_iter = 30
                                      );


# L'optimisation a eu ici un impact n√©gatif tr√®s faible : f1 macro de **70,64%** sur le jeu de validation, soit **-0,15%** , pour un temps de calcul d√©multipli√© (55min vs 3sec)

# In[226]:


item = pd.DataFrame([['base_TfIdf_RF_prepro_opti_', f1_score(y_val, base_TfIdf_RF_prepro_opti_.predict(X_val_prepro),average='macro')]], columns=['mod√®le', 'f1_macro_val'])

item


# In[227]:


r√©sultats = r√©sultats.append(item)
r√©sultats


# #### Bag of Words avec r√©gression logistique

# Une autre variante : on essaie un autre classifier, la r√©gression logistique
# 
# ![bow_pipeline_LR](images/Pipeline_BoW_LR_prepro.png)
# 

# In[228]:


tfidf_LR_pipeline = Pipeline(
    steps=[
        ('coltext', TextSelector('text')), 
        ("tfidf", TfidfVectorizer()),
        ("classifier", LogisticRegression(solver='liblinear', multi_class='auto')),
    ]
)


# In[229]:


list(bow_pipeline_LR.get_params().keys())


# In[230]:


pipe = tfidf_LR_pipeline


params = target_params( pipe , {
                                "use_idf": [True, False],
                                "ngram_range": [(1, 1), (1, 2), (1,3)],
                                "class_weight": [None, 'balanced']
                                })    

TfIdf_LR_prepro_opti_ = trainPipelineMlFlow(
                                mlf_XP = "Rapport",
                                xp_name_iter = "TfIdf_LR_prepro_opti", 
                                pipeline = pipe, 
                                X_train = X_train_prepro, y_train = y_train, X_test = X_val_prepro, y_test = y_val,
                                target_col = 'sentiment',
                                fixed_params = target_params(pipe, {'n_jobs':-1,'random_state':42}),
                                use_opti = True,
                                iterable_params = params,
                                n_iter=30
                                );


# Le classifier LogisticRegression avec les donn√©es retrait√©es performe moins bien que le RandomForest (**69,86%** sur le jeu de validation).

# In[231]:


item = pd.DataFrame([['TfIdf_LR_prepro_opti_', f1_score(y_val, TfIdf_LR_prepro_opti_.predict(X_val_prepro),average='macro')]], columns=['mod√®le', 'f1_macro_val'])

item


# In[232]:


r√©sultats = r√©sultats.append(item)
r√©sultats


# Afin de v√©rifier si les donn√©es retrait√©es apprortent quelque chose on relance le m√™me pipeline avec les jeux d'origine

# In[233]:


pipe = tfidf_LR_pipeline


params = target_params( pipe , {
                                "use_idf": [True, False],
                                "ngram_range": [(1, 1), (1, 2), (1,3)],
                                "class_weight": [None, 'balanced']
                                })    

TfIdf_LR_opti_ = trainPipelineMlFlow(
                        mlf_XP = "Rapport",
                        xp_name_iter = "TfIdf_LR_opti", 
                        pipeline = pipe, 
                        X_train = X_train, y_train = y_train, X_test = X_val, y_test = y_val,
                        target_col = 'sentiment',
                        fixed_params = target_params(pipe, {'n_jobs':-1,'random_state':42}),
                        use_opti = True,
                        iterable_params = params,
                        n_iter=30
                        );


# On observe un comportement diff√©rent avec la r√©gression logistice. Dans ce cas, c'est l'utilisation du jeu d'origine (avec le tokeniser par d√©faut de Tf Idf) qui apport de meilleurs r√©sultats (**69,99%** sur le jeu de validation), sans toutefois √©galer les performances du RandomForest

# In[234]:


item = pd.DataFrame([['TfIdf_LR_opti_', f1_score(y_val, TfIdf_LR_opti_.predict(X_val),average='macro')]], columns=['mod√®le', 'f1_macro_val'])

item


# In[235]:


r√©sultats = r√©sultats.append(item)
r√©sultats


# Ainsi les m√©thodes classiques nous aurons permi de gagner 3 points de f1 macro, le leader actuel √©tant le mod√®le RandomForest avec un simple BagOfWords (TfIdf=False) et optimis√© dans ses param√®tres sur le jeu de donn√©es pr√©trait√©.

# In[236]:


r√©sultats_tri√© = r√©sultats.sort_values(by='f1_macro_val',ascending=False)
r√©sultats_tri√©


# #### Optimisation du seuil de d√©cision pour maximiser le f1

# On peut aussi tirer avantage de la m√©trique utilis√©e pour l'√©valuation. En effet, parmi les 3 cat√©gories recherch√©es (`negative`, `neutral` et `positive`) il existe une gradation et en d√©finitive, on est surtout int√©ress√©s √† d√©terminer qsi un commentaire est positif ou n√©gatif. La classification neutre √©tant une cat√©gorie "par d√©faut" sans marqueur fort. 
# 
# Strat√©gie : on maximise sur le jeu d'entra√Ænement le seuil pour la d√©cision positive, puis sur les non positifs, on maximise le seuil pour les n√©gatifs, le reste est neutre

# In[237]:


# permet de prendre une d√©cision √† partir d'un seuil
def to_labels(pos_probs, threshold):
    return (pos_probs >= threshold).astype('int')


# In[238]:


def find_optimal_f1_thresholds(pipe, X, y):
    
    probs = pipe.predict_proba(X)
    
    # On commence par travailler les pr√©dictions positives
    pos_probs = probs[:,2]
    # On d√©finit une √©chelle de seuils
    thresholds = np.arange(0, 1, 0.001)
    # On √©value le f1 pour chaque seuil
    scores = [f1_score([(1 if i==1 else 0) for i in y ], to_labels(pos_probs, t)) for t in thresholds]
    # On r√©cup√®re le seuil optimal pour la cat√©gorie positive
    ix = np.argmax(scores)

    
    res = {'pos_threshold' : thresholds[ix], 'pos_f1' : scores[ix] }
    
    # On continue avec les pr√©dictions n√©gatives
    neg_probs = probs[:,0]
    # On d√©finit une √©chelle de seuils
    thresholds = np.arange(0, 1, 0.001)
    # On √©value le f1 pour chaque seuil
    scores = [f1_score([(1 if i==-1 else 0) for i in y ], to_labels(neg_probs, t)) for t in thresholds]
    # On r√©cup√®re le seuil optimal pour la cat√©gorie positive
    ix = np.argmax(scores)

    
    res.update({'neg_threshold' : thresholds[ix], 'neg_f1' : scores[ix] })
    
    return res
    


# In[239]:


# start√©gie : on commence par d√©cider si positif,
# sur les non positifs

def sentiment_predict(pipe, X, dict_thres):
    '''
    strat√©gie :  on commence par d√©cider si positif,
                 sur les non positifs, on d√©cide si n√©gatifs,
                 les restants sont neutres
    '''
    
    
    
    seuil_pos=dict_thres['pos_threshold']
    seuil_neg=dict_thres['neg_threshold']

    probs = pipe.predict_proba(X)

    y_test_pred_pos = to_labels(probs[:,2], seuil_pos)
    y_test_pred_neg = to_labels(probs[:,0], seuil_neg)

    y_test_pred = y_test_pred_pos
    y_test_pred[(y_test_pred_pos==0)] = -y_test_pred_neg[(y_test_pred_pos==0)]
    return y_test_pred


# In[240]:


thres = find_optimal_f1_thresholds(base_TfIdf_RF_prepro_opti_, X_train_prepro, y_train['sentiment'])


# In[241]:


thres


# In[242]:


y_val_pred = sentiment_predict(base_TfIdf_RF_prepro_opti_, X_val_prepro,thres)


# In[243]:


f1_score(y_val, y_val_pred, average='macro')


# Le gain est modeste (**+0,30%**), mais reste dans les ordres de grandeur des optimisations de pipeline

# In[244]:


item = pd.DataFrame([['TfIdf_LR_opti_modif_seuil', f1_score(y_val, y_val_pred, average='macro')]], columns=['mod√®le', 'f1_macro_val'])

item


# In[245]:


r√©sultats = r√©sultats.append(item).sort_values(by='f1_macro_val',ascending=False)
r√©sultats


# ### Approches par transformers pr√© entra√Æn√©s 

# Le traite√πent du langage est un sujet notoirement complexe. Les approches classiques utilis√©es pr√©c√©dement s'appuyaient sur des approches fr√©quentistes (Tf Idf / Bag Of Words) et le retraitement manuel de certains aspects (URL, utilisateurs cit√©s etc.).
# 
# Une m√©thode qui a fait ses preuves ces derni√®res ann√©es est l'utilisation du Deep Learning de mani√®re g√©n√©rale et de l'architecture [BERT](https://fr.wikipedia.org/wiki/BERT_(mod%C3%A8le_de_langage)) en particulier.
# 
# <div>
# <img src=https://www.codemotion.com/magazine/wp-content/uploads/2020/05/bert-google.png width="400"/>
# </div>
# 
# Dans un mode de fonctionnement optimal, on devrait reprndre BERT et r√©entrainer la derni√®re couche uniquement pour le sujet de classification √©tudi√©.
# Pour des raisons de temps et de comp√©tence, ce n'est pas l'approche prise ici.
# 
# Dans ce rapport, nous avons repris un mod√®le pr√©-entrain√© d√©riv√© de BERT et mis √† disposition par [HuggingFace](https://huggingface.co/)
# 
# ![HuggingFace](https://huggingface.co/front/assets/huggingface_logo.svg)
# 
# Plus pr√©cisement, le choix s'est port√© sur le mod√®le [roBERTa](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment) optimis√© pour la t√¢che de classification de sentiment de Twitter
# 
# La difficult√© principale rencontr√©e pour utiliser ce mod√®le a √©t√© d'adapter le fonctionnement du docker compose pour permettre l'acc√®s aux ressources GPU du PC. Dans l'alternative, le temps de traitement √©tait r√©dhibitoire. 

# #### Mise en place de l'environnement

# In[246]:


import torch
torch.cuda.is_available()


# In[247]:


from transformers import AutoModelForSequenceClassification
from transformers import TFAutoModelForSequenceClassification
from transformers import AutoTokenizer, AutoConfig
from transformers import pipeline


import numpy as np
from scipy.special import softmax
import csv
import urllib.request


# In[248]:



# Preprocess text (username and link placeholders)
def preprocess(text):
    new_text = []


    for t in text.split(" "):
        t = '@user' if t.startswith('@') and len(t) > 1 else t
        t = 'http' if t.startswith('http') else t
        new_text.append(t)
    return " ".join(new_text)


# Les mod√®les sont assez lourds (environ 500Mo)
# 
# Apr√®s avoir √©t√© t√©l√©charg√©, il est important de r√©utiliser les documents sur disque

# In[249]:


task='sentiment'
MODEL = f"cardiffnlp/twitter-roberta-base-{task}"

model = AutoModelForSequenceClassification.from_pretrained('/mnt/pretrained_models/'+MODEL)
tokenizer = AutoTokenizer.from_pretrained('/mnt/pretrained_models/'+MODEL)
config = AutoConfig.from_pretrained('/mnt/pretrained_models/'+MODEL)


# In[250]:


# download label mapping
labels=[]
mapping_link = f"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt"
with urllib.request.urlopen(mapping_link) as f:
    html = f.read().decode('utf-8').split("\n")
    csvreader = csv.reader(html, delimiter='\t')
labels = [row[1] for row in csvreader if len(row) > 1]


# In[251]:


nlp=pipeline("sentiment-analysis", model=model, tokenizer=tokenizer, device=0, return_all_scores=True)


# In[252]:


def TorchTwitterRoBERTa_Pred(text = "Good night üòä"):
    text = preprocess(text)
    otpt = nlp(text)[0]
#    otpt = (list(otpt[i].values())[1] for i in range(len(otpt)))
    neg = otpt[0]['score']
    neu = otpt[1]['score']
    pos = otpt[2]['score']
    
#    NewName = {0:'roBERTa-neg', 1:'roBERTa-neu', 2:'roBERTa-pos'}
#    otpt = pd.json_normalize(otpt).transpose().rename(columns=NewName).reset_index().drop([0]).drop(columns=['index'])
    return neg, neu, pos


# In[253]:


test = TorchTwitterRoBERTa_Pred()
test


# La partie pr√©c√©dente permettait de transcrire le code de Huggingface.
# 
# N√©anmoins l'utilisation pour faire des pr√©dictions sur l'int√©gralit√© d'une base peut vite √™tre longue. Le code suivant permet d'optimiser le temps de parcours des donn√©es.

# In[254]:


def run_loopy_roBERTa(df):
    v_neg, v_neu, v_pos = [], [], []
    for _, row in df.iterrows():
        v1, v2, v3 = TorchTwitterRoBERTa_Pred(row.values[0])
        v_neg.append(v1)
        v_neu.append(v2)
        v_pos.append(v3)
    df_result = pd.DataFrame({'roBERTa_neg': v_neg,
                              'roBERTa_neu': v_neu,
                              'roBERTa_pos': v_pos})
    df_result.set_index(df.index)
    return df_result


# Afin d'utiliser la logique des pipelines, on cr√©e une classe sp√©cifique :

# In[255]:


class clTwitterroBERTa(BaseEstimator, TransformerMixin):
    
    def __init__(self, field):
        self.field = field
        
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        res = run_loopy_roBERTa(X[[self.field]])
        return res


# #### roBERTa Twitter Sentiment

# On dispose d√©sormais de tous les √©l√©ments n√©cessaires.
# roBERTa ayant √©t√© entra√Æn√© sur 58M de tweets en anglais, nous n'avons pas √† appliquer de preprocessing en dehors de la standardisation des adresses et utilisateurs pr√©vus par d√©faut dans le code de Huggingface.
# 
# ![roBERTa_RF](images/Pipeline_roBERTa_RF.png)
# 

# In[256]:


roBERTa_pipe=Pipeline([
                     ('roBERTa', clTwitterroBERTa(field='text'))
                    ])


# In[257]:


roBERTa_RF_Pipe = Pipeline(
    steps=[
        ('roBERTa', roBERTa_pipe),
        ("classifier", RandomForestClassifier(n_jobs=-1))
    ]
)


# In[258]:


pipe = roBERTa_RF_Pipe


roBERTa_RF_= trainPipelineMlFlow(
                    mlf_XP = "Rapport",
                    xp_name_iter = "roBERTa_RF", 
                    pipeline = pipe, 
                    X_train = X_train, y_train = y_train, X_test = X_val, y_test = y_val,
                    target_col = 'sentiment',
                    fixed_params = target_params(pipe, {'n_jobs':-1,'random_state':42})
                    );


# In[259]:


item = pd.DataFrame([['roBERTa_RF_', f1_score(y_val, roBERTa_RF_.predict(X_val), average='macro')]], columns=['mod√®le', 'f1_macro_val'])

item


# In[260]:


r√©sultats = r√©sultats.append(item).sort_values(by='f1_macro_val',ascending=False)
r√©sultats


# Sans optimisation, le mod√®le utilisant roBERTa tweet ne se place qu'en 4√®me position, ce qui est en de√ß√† des attentes a priori.

# Par ailleurs, la quantit√© de m√©moire vive √† disposition sur la carte √©tant limit√©e, il n'a pas √©t√© possible d'effectuer une optimisation directe du pipeline, celle-ci cr√©ant des d√©passements de m√©moire.
# 
# C'est pourquoi la phase de pr√©diction par roBERTa tweet a √©t√© isol√©e (celle-ci ne pr√©sentant par ailleurs aps de possibilit√© de param√©trage) afin de laisser le seul classifier dans l'optimisation.

# #### roBERTa Twitter Sentiment optimis√©

# In[261]:


import gc

gc.collect()

torch.cuda.empty_cache()


# In[179]:


X_train_roBERTa = roBERTa_pipe.transform(X_train)
X_val_roBERTa = roBERTa_pipe.transform(X_val)
X_test_roBERTa = roBERTa_pipe.transform(X_test)


# In[237]:


X_train_roBERTa = X_train_roBERTa.set_index(X_train.index)
X_val_roBERTa = X_val_roBERTa.set_index(X_val.index)
X_test_roBERTa = X_test_roBERTa.set_index(X_test.index)
X_train_roBERTa


# In[238]:


X_train_roBERTa.to_parquet('/mnt/data/interim/X_train_roBERTa.gzip',compression='gzip')
X_val_roBERTa.to_parquet('/mnt/data/interim/X_val_roBERTa.gzip',compression='gzip')
X_test_roBERTa.to_parquet('/mnt/data/interim/X_test_roBERTa.gzip',compression='gzip')


# In[13]:


X_train_roBERTa = pd.read_parquet('/mnt/data/interim/X_train_roBERTa.gzip')
X_val_roBERTa = pd.read_parquet('/mnt/data/interim/X_val_roBERTa.gzip')
X_test_roBERTa = pd.read_parquet('/mnt/data/interim/X_test_roBERTa.gzip')


# ![roBERTa_prepro](images/Pipeline_roBERTa_prepro_RF.png)

# In[262]:


roBERTa_RF = Pipeline(
    steps=[
        ("classifier", RandomForestClassifier(n_jobs=-1))
    ]
)


# In[263]:


pipe = roBERTa_RF

params = target_params(pipe, {
    "bootstrap": [True, False],
    "class_weight": ["balanced", None],
    "n_estimators": [100, 300, 500, 800, 1200],
    "max_depth": [5, 8, 15, 25, 30],
    "min_samples_split": [2, 5, 10, 15, 100],
    "min_samples_leaf": [1, 2, 5, 10]
})


roBERTa_RF_opti_ = trainPipelineMlFlow(
                    mlf_XP = "Rapport",
                    xp_name_iter="roBERTa_RF_opti", 
                    pipeline = pipe, 
                    X_train = X_train_roBERTa, y_train = y_train, X_test = X_val_roBERTa, y_test = y_val,
                    target_col = 'sentiment',
                    fixed_params = target_params(pipe, {'n_jobs':-1,'random_state':42}),
                    use_opti = True,
                    iterable_params=params,
                    n_iter=30
                    );


# Le mod√®le, une fois optimis√©, arrive en haut du classement avec un gain de persque **+4%** de f1 pour atteindre **74,66%** sur le jeu de validation

# In[264]:


item = pd.DataFrame([['roBERTa_RF_opti_', f1_score(y_val, roBERTa_RF_opti_.predict(X_val_roBERTa), average='macro')]], columns=['mod√®le', 'f1_macro_val'])

item


# In[265]:


r√©sultats = r√©sultats.append(item).sort_values(by='f1_macro_val',ascending=False)
r√©sultats


# #### Essai combinaison de diff√©rentes m√©thodes

# Afin de gagner encore en performance, il est possible de combiner plusieurs outils d'estimation de sentimenst a priori. Ces transformations ne relevant pas des m√™mes strat√©gies, elles capturent des √©l√©ments l√©g√®rement diff√©rents.
# 
# Les m√©thodes s√©lectionn√©es ici pour leur simplicit√© d'utilisation sont :
# - [TextBlob](https://textblob.readthedocs.io/en/dev/quickstart.html)
# - [Vader](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf)
# 
# ![roBERTa_Blob_Vader](images/Pipeline_roBERTa_Blob_Vader_prepro_RF.png)
# 
# 

# In[266]:


class Blob(BaseEstimator, TransformerMixin):
    def __init__(self, field):
        self.field = field
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        X[['polarity', 'subjectivity']] =  X[self.field].apply(lambda x:TextBlob(x).sentiment).apply(pd.Series)
        return X[['polarity', 'subjectivity']]


# In[187]:


blob_pipe=Pipeline([
                     ('blob', Blob(field='text'))
                    ])


# In[188]:


X_train_Blob=blob_pipe.transform(X_train)
X_val_Blob=blob_pipe.transform(X_val)
X_test_Blob=blob_pipe.transform(X_test)


# In[189]:


X_train_Blob.head()


# In[190]:


X_train_Blob.to_parquet('/mnt/data/interim/X_train_Blob.gzip',compression='gzip')
X_val_Blob.to_parquet('/mnt/data/interim/X_val_Blob.gzip',compression='gzip')
X_test_Blob.to_parquet('/mnt/data/interim/X_test_Blob.gzip',compression='gzip')


# In[267]:


X_train_Blob = pd.read_parquet('/mnt/data/interim/X_train_Blob.gzip')
X_val_Blob = pd.read_parquet('/mnt/data/interim/X_val_Blob.gzip')
X_test_Blob = pd.read_parquet('/mnt/data/interim/X_test_Blob.gzip')


# On v√©rifie que TextBlob et roBERTa ne capturent pas les m√™mes √©l√©ments.
# 
# TextBlob fournissant un indicateur global, on approxime les sentiments de rBERTa comme `positive` - `negative`

# In[268]:


X =pd.DataFrame(columns=['roBERTa_sent'])
X['roBERTa_sent'] = X_train_roBERTa['roBERTa_pos']- X_train_roBERTa['roBERTa_neg']
X2 = pd.concat([X, X_train_Blob[['polarity']]], axis=1)
X2.corr()


# In[269]:


fig = px.scatter(x = X_train_roBERTa['roBERTa_pos']- X_train_roBERTa['roBERTa_neg'], 
                 y = X_train_Blob['polarity'],
                labels = {
                     'x': 'roBERTa',
                     'y' : 'TextBLob - polarity',
                 },
                title = 'Comparaison des sentiments roBERTa vs TextBlob')
fig.show()


# In[270]:


class Vader(BaseEstimator, TransformerMixin):
    def __init__(self, field):
        self.field = field
        sid = SentimentIntensityAnalyzer()
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        sid = SentimentIntensityAnalyzer()
        X[['neg', 'neu', 'pos', 'compound']] =  X[self.field].apply(sid.polarity_scores).apply(pd.Series)
        return X[['neg', 'neu', 'pos', 'compound']]


# In[208]:


vader_pipe=Pipeline([
                     ('vader', Vader(field='text'))
                    ])


# In[209]:


X_train_Vader=vader_pipe.transform(X_train)
X_val_Vader=vader_pipe.transform(X_val)
X_test_Vader=vader_pipe.transform(X_test)


# In[210]:


X_train_Vader.head()


# In[211]:


X_train_Vader.to_parquet('/mnt/data/interim/X_train_Vader.gzip',compression='gzip')
X_val_Vader.to_parquet('/mnt/data/interim/X_val_Vader.gzip',compression='gzip')
X_test_Vader.to_parquet('/mnt/data/interim/X_test_Vader.gzip',compression='gzip')


# In[271]:


X_train_Vader = pd.read_parquet('/mnt/data/interim/X_train_Vader.gzip')
X_val_Vader = pd.read_parquet('/mnt/data/interim/X_val_Vader.gzip')
X_test_Vader = pd.read_parquet('/mnt/data/interim/X_test_Vader.gzip')


# On v√©rifie de la m√™me mani√®re que Vader et roBERTa ne capturent pas les m√™mes √©l√©ments.

# Pour les positifs

# In[272]:


X_pos = pd.concat([X_train_roBERTa[['roBERTa_pos']], X_train_Vader[['pos']]], axis=1)
X_pos.corr()


# In[273]:


fig = px.scatter(x = X_train_roBERTa['roBERTa_pos'], 
                 y = X_train_Vader['pos'],
                labels = {
                     'x': 'roBERTa',
                     'y' : 'Vader',
                 },
                title = 'Comparaison des sentiments roBERTa vs Vaders - positive')
fig.show()


# Pour les neutres

# In[274]:


X_pos = pd.concat([X_train_roBERTa[['roBERTa_neu']], X_train_Vader[['neu']]], axis=1)
X_pos.corr()


# In[275]:


fig = px.scatter(x = X_train_roBERTa['roBERTa_neu'], 
                 y = X_train_Vader['neu'],
                labels = {
                     'x': 'roBERTa',
                     'y' : 'Vader',
                 },
                title = 'Comparaison des sentiments roBERTa vs Vaders - neutral')
fig.show()


# Pour les n√©gatifs

# In[276]:


X_pos = pd.concat([X_train_roBERTa[['roBERTa_neg']], X_train_Vader[['neg']]], axis=1)
X_pos.corr()


# In[277]:


fig = px.scatter(x = X_train_roBERTa['roBERTa_neg'], 
                 y = X_train_Vader['neg'],
                labels = {
                     'x': 'roBERTa',
                     'y' : 'Vader',
                 },
                title = 'Comparaison des sentiments roBERTa vs Vaders - negative')
fig.show()


# On peut alors calculer la base agr√©g√©e

# In[278]:


X_train_compound = pd.concat([X_train_roBERTa, X_train_Blob, X_train_Vader], axis=1)
X_val_compound = pd.concat([X_val_roBERTa, X_val_Blob, X_val_Vader], axis=1)
X_test_compound = pd.concat([X_test_roBERTa, X_test_Blob, X_test_Vader], axis=1)


# In[15]:


X_train_compound.head()


# In[16]:


X_val_compound.head()


# :::{tip}
# Comme on travaille avec des arbres il n'y a pas besoin de renormer / standardiser les diff√©rentes colonnes
# :::

# In[279]:


pipe = roBERTa_RF

params = target_params(pipe, {
    "bootstrap": [True, False],
    "class_weight": ["balanced", None],
    "n_estimators": [100, 300, 500, 800, 1200],
    "max_depth": [5, 8, 15, 25, 30],
    "min_samples_split": [2, 5, 10, 15, 100],
    "min_samples_leaf": [1, 2, 5, 10]
})


roBERTa_Blob_Vader_RF_opti_ = trainPipelineMlFlow(
                                    mlf_XP="Rapport",
                                    xp_name_iter="roBERTa_Blob_Vader_RF_opti", 
                                    pipeline = pipe, 
                                    X_train = X_train_compound, y_train = y_train, X_test = X_val_compound, y_test = y_val,
                                    target_col = 'sentiment',
                                    fixed_params = target_params(pipe, {'n_jobs':-1,'random_state':42}),
                                    use_opti = True,
                                    iterable_params = params,
                                    n_iter = 30
                                    );


# In[280]:


item = pd.DataFrame([['roBERTa_Blob_Vader_RF_opti_', f1_score(y_val, roBERTa_Blob_Vader_RF_opti_.predict(X_val_compound), average='macro')]], columns=['mod√®le', 'f1_macro_val'])

item


# In[281]:


r√©sultats = r√©sultats.append(item).sort_values(by='f1_macro_val',ascending=False)
r√©sultats


# :::{note}
# L'utilisation de Vader et Blob en soutien de roBERTa a permi de gagner 1 point de f1 sur le jeu de validation         
# :::

# ### Essai xgboost sur combinaison de m√©thodes

# Dans ce dernier essai on remplace le RandomClassifier par un XGBoost
# 
# ![xgb](images/Pipeline_roBERTa_Blob_Vader_prepro_xgb.png)
# 

# In[282]:


import xgboost as xgb


# In[18]:


roBERTa_xgb = Pipeline(
    steps=[
        ("classifier", xgb.XGBClassifier())
    ]
)


# :::{tip}
# Dans l'exemple ci-dessous on utilise explicitement le GPU pour acc√©l√©rer les calculs (`tree_method = 'gpu_hist'` et `gpu_id=0`) .
# Le gain est saisissant : dans une premi√®re version qui n'utilisait que le CPU, le mod√®le tournait en 3h30, contre un peu plus de 4 min ici, soit un gain de temps de presque 1:52!
# :::

# In[19]:


pipe = roBERTa_xgb

params = target_params(pipe, {
     "eta"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,
     "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],
     "min_child_weight" : [ 1, 3, 5, 7 ],
     "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
     "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ]
     })


roBERTa_xgb_opti_ = trainPipelineMlFlow(
                    mlf_XP="DSA_Tweets",
                    xp_name_iter="roBERTa - xgb - opti", 
                    pipeline = pipe, 
                    X_train = X_train_compound, y_train = y_train, X_test = X_val_compound, y_test = y_val,
                    target_col = 'sentiment',
                    fixed_params = target_params(pipe, {'n_jobs':-1,'random_state':42, 'gpu_id':0, 'tree_method' : 'gpu_hist'}),
                    use_opti = True,
                    iterable_params=params,
                    n_iter=20
                    )


# On essaye de comprendre les 27 tweets n√©gatifs qui ont √©t√© pr√©dits positifs

# Le mod√®le XGBoost optimis√© √† partir des donn√©es augment√© se hisse √† la premi√®re place du podium

# In[286]:


item = pd.DataFrame([['roBERTa_xgb_opti_', f1_score(y_val, roBERTa_xgb_opti_.predict(X_val_compound), average='macro')]], columns=['mod√®le', 'f1_macro_val'])

item


# In[287]:


r√©sultats = r√©sultats.append(item).sort_values(by='f1_macro_val',ascending=False)
r√©sultats


# In[288]:


r√©sultats_fin = r√©sultats


# In[289]:


r√©sultats_fin


# On peut s'interroger sur les pr√©diction restantes qui sont positives, mais class√©es comme n√©gatives et inversement

# In[290]:


y_val_pred = roBERTa_xgb_opti_.predict(X_val_compound)
inpt = pd.concat([X_val, X_val_compound], axis=1)


# In[293]:


exemples_realNeg_predPos_fin = inpt[(y_val['sentiment']==-1) & (y_val_pred==1)]
exemples_realPos_predNeg_fin = inpt[(y_val['sentiment']==1) & (y_val_pred==-1)]


# In[294]:


exemples_realNeg_predPos_fin


# Contrairement √† la premi√®re analyse, on observe que le taux de tweets vraisemblablement mal libell√©s semble plus important

# In[314]:


exemples_realPos_predNeg_fin


# On peut essayer de comprendre comment le mod√®le exploite les informations fournies pour prendre ses d√©cisions en s'appuyant sur SHAP

# In[315]:


#set the tree explainer as the model of the pipeline
explainer = shap.TreeExplainer(roBERTa_xgb_opti_.best_estimator_['classifier'])

#apply the preprocessing to x_test
#observations = pipeline['imputer'].transform(x_test)
observations = X_val_compound

#get Shap values from preprocessed data
shap_values = explainer.shap_values(observations)

#plot the feature importance
titres = {0 : 'Pr√©dictions n√©gatives', 1: 'Pr√©dictions neutres', 2 : 'Pr√©dictions positives'}
for i in range(3):
    shap.summary_plot(shap_values[i], observations, plot_type="bar", show=False)
    plt.title(titres[i])
    plt.show()
    


# :::{tip}
# Cette premi√®re analyse confirme que le mod√®le s'appuie principalement sur les pr√©diction de roBERTa tweet de la classification idoine pour prendre sa d√©cision. Les autres composantes ayant des contributions bien plus faibles. 
# :::

# On peut ensuite zoomer sur la mani√®re dont les pr√©dictions de roBERTa sont prises en compte dans le mod√®le :

# In[316]:


titres = {0 : 'Pr√©dictions n√©gatives', 1: 'Pr√©dictions neutres', 2 : 'Pr√©dictions positives'}
axe = {0 : 'roBERTa_neg', 1: 'roBERTa_neu', 2 : 'roBERTa_pos'}
for i in range(3):
    shap.dependence_plot(axe[i], shap_values[i], X_val_compound, show=False)
    plt.title(titres[i])
    plt.show()


# ## Soumission finale

# On r√©entraine les 2 mod√®les finalistes sur le jeu de validation sur l'int√©gralit√© de `tain + val` et on √©value sur le jeu de test

# :::{warning}
# Ici on r√©ajuste uniquement les mod√®les sans explorer de nouvelles vvaleur d'hyperparam√®tres
# :::

# In[317]:


y_train_tot = pd.concat([y_train, y_val], axis=0)


# In[318]:


X_train_compound_tot = pd.concat([X_train_compound, X_val_compound], axis=0)


# In[32]:


X_train_compound_tot


# In[320]:


pipe = roBERTa_RF

params = target_params(pipe, {
    'n_jobs':-1,
    'random_state':42,
    'n_estimators': 500, 
    'classifier__min_samples_split': 15, 
    'classifier__min_samples_leaf': 10, 
    'classifier__max_depth': 15, 
    'classifier__class_weight': None, 
    'classifier__bootstrap': True
})


roBERTa_Blob_Vader_RF_opti_tot_ = trainPipelineMlFlow(
                                        mlf_XP="Rapport",
                                        xp_name_iter="roBERTa_Blob_Vader_RF_opti_tot", 
                                        pipeline = pipe, 
                                        X_train = X_train_compound_tot, y_train = y_train_tot, X_test = X_test_compound, y_test = y_test,
                                        target_col = 'sentiment',
                                        fixed_params = params,
                                        use_opti = False,
                                        );


# In[321]:


item = pd.DataFrame([['roBERTa_Blob_Vader_RF_opti_', f1_score(y_test, roBERTa_Blob_Vader_RF_opti_tot_.predict(X_test_compound), average='macro')]], columns=['mod√®le', 'f1_macro_test'])

item


# In[322]:


res_fin=item


# In[323]:


pipe = roBERTa_xgb

params = target_params(pipe, {
                'classifier__n_jobs': -1, 
                'classifier__random_state': 42, 
                'classifier__gpu_id': 0, 
                'classifier__tree_method': 'gpu_hist', 
                'classifier__min_child_weight': 7, 
                'classifier__max_depth': 5, 
                'classifier__gamma': 0.1, 
                'classifier__colsample_bytree': 0.7
     })


roBERTa_xgb_opti_tot_ = trainPipelineMlFlow(
                    mlf_XP="Rapport",
                    xp_name_iter="roBERTa_xgb_opti_tot", 
                    pipeline = pipe, 
                    X_train = X_train_compound_tot, y_train = y_train_tot, X_test = X_test_compound, y_test = y_test,
                    target_col = 'sentiment',
                    fixed_params = params,
                    use_opti = False
                    );


# In[324]:


item = pd.DataFrame([['roBERTa_xgb_opti_', f1_score(y_test, roBERTa_xgb_opti_tot_.predict(X_test_compound), average='macro')]], columns=['mod√®le', 'f1_macro_test'])

item


# In[325]:


res_fin=res_fin.append(item)


# In[326]:


res_fin2=pd.merge(r√©sultats, res_fin, how='left', on='mod√®le')
res_fin2


# In[327]:


r√©sultats.to_parquet('/mnt/data/processed/r√©sultats.gzip',compression='gzip')
res_fin.to_parquet('/mnt/data/processed/res_fin.gzip',compression='gzip')
res_fin2.to_parquet('/mnt/data/processed/res_fin2.gzip',compression='gzip')


# :::{note}
# Le mod√®le final pr√©sente un f1 macro de **0.76** sur le jeu de test.
# 
# Les r√©sultats sont en ligne avec ceux obtenus sur le jeu de validation. N√©anmoins le fait de ne pas avoir regarder les r√©sultats sur le jeu de test avant cette √©tape assure que nous n'avons pas pu faire de leakage d'une mani√®re ou d'une autre. Cette d√©marche √©mule aussi celle qui existerait dans un cas industriel o√π une √©quipe s√©par√©e serait en charge de d√©finir un jeu de test permettant de tester le mod√®le dans des conditions d√©grad√©es par rapport √† son domaine d'entra√Ænement pour en assurer la stabilit√© en production.
# :::

# In[ ]:




