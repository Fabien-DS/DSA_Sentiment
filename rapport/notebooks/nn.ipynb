{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "major-scroll",
   "metadata": {},
   "source": [
    "# Essai NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-router",
   "metadata": {},
   "source": [
    "source : https://scottmduda.medium.com/fine-tuning-language-models-for-sentiment-analysis-91db72396549\n",
    "\n",
    "github : https://github.com/dontmindifiduda/financial_statement_sentiment_analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unlike-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, DistilBertTokenizer, RobertaModel, RobertaTokenizer\n",
    "from transformers import AutoConfig, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import mlflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-april",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chronic-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Importe les données\n",
    "\n",
    "#df\n",
    "df_train=pd.read_parquet('/mnt/data/interim/df_train.gzip')\n",
    "df_val=pd.read_parquet('/mnt/data/interim/df_val.gzip')\n",
    "df_test=pd.read_parquet('/mnt/data/interim/df_test.gzip')\n",
    "\n",
    "#X\n",
    "X_train=pd.read_parquet('/mnt/data/interim/X_train.gzip')\n",
    "X_val=pd.read_parquet('/mnt/data/interim/X_val.gzip')\n",
    "X_test=pd.read_parquet('/mnt/data/interim/X_test.gzip')\n",
    "\n",
    "X_train_prepro=pd.read_parquet('/mnt/data/interim/X_train_prepro.gzip')\n",
    "X_val_prepro=pd.read_parquet('/mnt/data/interim/X_val_prepro.gzip')\n",
    "X_test_prepro=pd.read_parquet('/mnt/data/interim/X_test_prepro.gzip')\n",
    "\n",
    "#y\n",
    "y_train=pd.read_parquet('/mnt/data/interim/y_train.gzip')\n",
    "y_val=pd.read_parquet('/mnt/data/interim/y_val.gzip')\n",
    "y_test=pd.read_parquet('/mnt/data/interim/y_test.gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-reporter",
   "metadata": {},
   "source": [
    "## Set Device and Apply Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "technological-flesh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6ea6d5c3b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "RANDOM_SEED = 73\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spectacular-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_val, df_test], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "perfect-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['num_char'] = df['text'].apply(len)\n",
    "df['num_words'] = df['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "surgical-henry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuMAAAHjCAYAAACJlRE5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA5OElEQVR4nO3de1xVdb7/8fcGvOFWdIOIiKmoaZqIioqXlJQzNjrTeLLpmGlq2WROdjMv5S+z06R0UTlqTuMldSynOo3azHQnQk10xAteSzRvmRCXjSaicvv+/vDhPpGhWwO+Bq/n4+HjwV5777U/a9N68Gqx9sJhjDECAAAAUOl8bA8AAAAAVFfEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ6gWtu9e7e6d++u2rVrq0WLFrbHqTBJSUlyOBw6fvx4hb3GkSNH5HA49MUXX1TYa9hSlbcNgF3EOIBfhMaNGyslJUWS1LdvX61atapc1jt58mTVr19fX331lWf9P7Z8+XI5HI6f/Ld169ZymQOXd/ToUY0aNUrNmjVTrVq1FBISotjYWH366afl/lqtW7fWjBkzSi1r1qyZ0tPT1aNHj3J/vWvxxRdfyOFw6MiRI7ZHAfAz+dkeAACu5ODBgzpz5ow6d+6sgoICpaSkaOXKleWy7gMHDmjUqFFXPCru6+v7k0eVg4KCymUOlK2wsFCxsbFq1qyZVq1apRtuuEHfffedkpKSlJOTUykz+Pr6KiQkpFJeC0D1wpFxANe9jRs3qkePHvLz81NKSooCAwPVvHnzKz4vPT1dw4YNU4MGDVSnTh3FxMR4jmRfPO3g66+/1vTp0+VwOC45GvpjISEhl/zz8/OTMUaDBw9Wt27dVFhYKEkqKSlRbGys+vbtq+LiYknS//zP/ygyMlJOp1MhISEaNmyY0tPTPeu/eCrJBx98oJ49e6pOnTrq2rWr9u7dq71796pPnz7y9/dX9+7dtW/fPs/zli9fLj8/PyUkJKhDhw6qXbu2evToodTU1Mtuz8GDBzV06FA1aNBADRs21K9+9Svt3r3bc//333+vMWPGKCQkRLVq1VKzZs30xBNPXPF9P3LkiAYMGKA6deooPDxcb731lue+mJgY/eEPfyj1eGOMWrVqpeeff/4n17d3714dPHhQ8+bN0y233KLmzZure/fumjx5soYNG+Z5XGFhoWbMmKGWLVuqdu3a6tChg/7yl7+UWpfD4dDChQs1cuRI1atXT2FhYZo1a1ap+b7++ms999xznt9+HDly5JLTVC7eXrVqlQYOHCh/f3+1a9dO69at07fffqtBgwapbt26at++vTZs2HBV7/vF7+fGjRvVpUsX+fv7q2vXrp7f3Bw5ckS33HKLJKlly5ZyOByKiYm54vcFwHXKAMB1KiAgwAQEBJjatWubmjVrmoCAAOPv7298fX0995WlpKTEdO/e3XTq1Mls2LDB7Nq1y9x1112mQYMGJisryxQVFZn09HQTFhZmpkyZYtLT083p06d/cl3Lli0zvr6+l501MzPTNGnSxEycONEYY8yf/vQn43K5zLFjxzyPiY+PN59++qk5dOiQSU5ONj179jR9+/b13P/5558bSSYyMtJ89tlnZu/evSY6Otp07NjR3HLLLSYhIcHs27fP9O7d23Tv3r3UfA6Hw3Tu3NkkJSWZnTt3msGDB5vQ0FCTn59fat3ffPONMcaYjIwM07hxYzNu3Diza9cu89VXX5mHH37YuFwuk5mZaYwxZsKECSYiIsJs3rzZHD161GzcuNEsWrSozPfg8OHDRpJp0qSJeeONN8xXX31lpk2bZnx8fMz27duNMcasWrXKOJ3OUu91QkKC8fX1Nd9+++1Prvfbb781Pj4+5tlnnzXnz58v8/VHjRplOnbsaD7++GNz6NAh89Zbb5mAgACzZMkSz2MkmeDgYLNo0SJz8OBBs2DBAiPJJCQkGGOMycnJMS1atDATJ0406enpJj093RQVFXm2bcOGDaW2NTw83KxZs8bs37/fDBkyxISEhJgBAwaY1atXm/3795uhQ4easLAwU1BQ4PX7fvH7ecstt5j169ebL7/80tx2222mRYsWprCw0BQVFZn33nvPSDJbtmwx6enpJicnp8z3BcD1jRgHcN06fPiwOXz4sGncuLH5+9//bg4fPmx69Ohh4uPjPfeVJSEhwUgye/fu9Sw7d+6cCQkJMc8995xnWfPmzc3zzz9/2TmWLVtmJJm6deuW+vfj/xlITEw0vr6+ZsaMGcbPz8+sWbPmsuvdvn27kWSOHz9ujPm/YP7h89555x0jybz77rueZatXrzaSPEF7cb6LQWmMMW6329StW9cToj+O8Weffdb06NGj1DwlJSUmPDzczJ071xhjzO23325GjRp12W34oYuB+v/+3/8rtbxnz55mxIgRxpgL34OgoCCzePFiz/3Dhg0zt99++2XX/ec//9nUrVvX1K5d2/Tq1ctMnjzZbNmyxXP/oUOHjMPhMF9++WWp5z333HOmU6dOntuSzIQJE0o9pl27dmbq1Kme261atTLPPvvsT27bj2P84ntljDFbtmwxkswrr7ziWXbxe7x7925jjHfv+8Xv57Zt2zyP2bx5s5FkvvrqK2OMMRs2bDCSLrsPAPhl4JxxANetFi1aaNeuXSosLNTvfvc75efnKzU1Vf/4xz8UHBx82efu3btXgYGBat++vWdZrVq11KNHD+3du/eqZ/H19b3ktA+Hw1Hq9q233qqJEydqxowZGjdunIYMGVLq/qSkJM2aNUv79u3TyZMnVVJSIunChxObNm3qeVynTp08X188TzkiIuKSZZmZmXI6nZ7lPXv29HzdsGFD3XTTTWVua0pKirZt21bq+ZJ09uxZHThwQJI0fvx4DR06VFu3btWAAQN02223aeDAgfLxufwZjj+cQ5J69+6tzz77TNKF78Ho0aO1ePFijR07Vjk5OVqzZo3efffdy65z3LhxGjFihNavX68tW7boo48+0ssvv6xZs2ZpypQp2rp1q4wxioqKKvW8oqIi+fr6lloWGRlZ6nZoaKi+++67y75+Wa7meyV5975LF/7b+uG6Q0NDJUnfffed2rZte02zArg+EeMArksdOnTQ0aNHVVRUpMLCQgUEBKikpETnz59XeHi4JGnfvn264YYbKm2m1q1bX/b+4uJibdy4Ub6+vvr6669ljPEE+7FjxzRo0CCNHDlS06dPV1BQkI4fP67Y2FgVFBSUWk+NGjU8X198/k8tuxjz16KkpEQDBgzQggULLrkvICBAkjRw4EAdO3ZMH3/8sZKSkjRixAh17NhRn3322SWBezUefPBBzZ49W7t27VJiYqIaNWqkX//611d8ntPp1KBBgzRo0CDNmDFDY8eO1fTp0/X444973ovk5GT5+/uXet6P/6epZs2al9x/re/l1X6vvHnfJcnHx6fUe1we33MA1yc+wAnguvTBBx8oNTVV3bt31/PPP6/U1FTdeeeduv/++5WamqrU1FTP0cKf0qFDB+Xk5JT6oOP58+f173//WzfffHOFzDxjxgwdPHhQGzdu1JYtW/TSSy957ktJSdHZs2cVHx+v3r17q23bttd8NLYsmzdv9nx98uRJffnll6V+M/BDUVFR2rt3r8LCwtS6detS/xo1auR5nMvl0t13362//OUvev/997Vu3bpS7+mV5pAuBPIP52jdurX69++vxYsXa8mSJbrvvvuuKe5vuukmFRQU6NSpU+rataukC//T8+PtadWq1VWtt2bNmp4P3ZY3b993b2aUVGFzAqg8xDiA61Lz5s3VsmVL7dq1S//5n/+p1q1ba8+ePfrNb37jiRc/v7J/ude/f391795dw4cP18aNG7Vnzx7de++9OnfunB566KFrmikjI+OSf+fPn5ckrVu3TnFxcVqxYoV69OihRYsW6ZlnntGWLVskSW3atJHD4dDs2bN1+PBhrV27Vv/93/99TXP8FIfDocmTJ2v9+vXavXu37r33XtWrV0/Dhw//ycc//PDDKi4u1u9+9ztt2LBBR44c0RdffKFp06YpOTlZkjRt2jStXr1a+/fv14EDB/Tmm2/K6XRe8bcRS5cu1apVq5SWlqbp06dr06ZNl1yF5cEHH9SiRYv05ZdfauzYsZdd344dO/Tb3/5W77zzjvbs2aNDhw7p7bff1ksvvaTevXurUaNGat26te677z498MADWrlypQ4ePKidO3fq9ddf14svvngV7+SFK5Rs3LhRx44dU3Z2drkejfbmffdG8+bN5ePjow8++ECZmZk6depUuc0IoHIR4wCuWzt27FDNmjV100036dSpU9q9e7f69u3r1XMdDofWrl2rdu3aeS47mJGRoU8//fSarg1eXFysJk2aXPLvn//8p9xut0aMGKFHH31UAwcOlCTdddddGj16tO6++26dPn1aERERmj9/vv7yl7+offv2euWVVxQfH3/Vc5TFx8dHM2fO1IMPPqioqChlZGTo/fffv+SUjYsaN26sTZs2KSgoSHfccYfatm2re+65R0ePHlWTJk0kSbVr19b06dPVtWtXRUVFadeuXfrwww9LnU7xU+Li4rRo0SJFRERo5cqVeuONN9SlS5dSjxkyZIgCAgJ02223qVmzZpddX7NmzdS6dWvNnDlTvXv3VseOHfXMM89o1KhR+sc//uF53KJFi/T444/rhRdeUPv27TVgwACtWLHCc1qTt5577jmdPHlSbdu2VaNGjXTs2LGrev7lePO+e7ueWbNmKS4uTk2aNNHvfve7cpsRQOVyGGOM7SEAANdu+fLlGjt2rIqKimyP4rWcnByFhYXprbfeIiQBVGscGQcAVJrCwkJlZGRo2rRpatq0qX7729/aHgkArCLGAQCVZuPGjWrSpIk++eQTrVix4oqXSQSAqo7TVAAAAABLOCQBAAAAWEKMAwAAAJYQ4wAAAIAlZf/FjGrixIkTtkcAAABAFXa5vxjNkXEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACzxsz1AVZQ+aaztEYBy0eTlJbZHAACgSuPIOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGCJX2W8yMKFC7V9+3YFBARo9uzZkqSVK1dq27Zt8vPzU+PGjTV+/HjVrVtXkrRmzRolJibKx8dHY8aMUWRkpCQpNTVVy5YtU0lJiQYMGKAhQ4ZIkjIzMxUfH6/Tp08rPDxcEyZMkJ9fpWwaAAAAcM0q5ch4TEyMnn766VLLIiIiNHv2bL3yyitq0qSJ1qxZI0k6fvy4kpOTNWfOHE2bNk1Lly5VSUmJSkpKtHTpUj399NOaO3euNm7cqOPHj0uS3njjDQ0ePFjz589X3bp1lZiYWBmbBQAAAPwslRLj7du3l9PpLLWsU6dO8vX1lSTdeOONcrvdkqSUlBT16tVLNWrUUHBwsEJCQnTw4EEdPHhQISEhaty4sfz8/NSrVy+lpKTIGKO9e/cqOjpa0oXwT0lJqYzNAgAAAH6W6+Kc8cTERM+pKG63W4GBgZ77XC6X3G73JcsDAwPldrt1+vRp+fv7e8L+4uMBAACA6531E6tXr14tX19f3XLLLZXyegkJCUpISJAkxcXFKSgoqNxfI73c1wjYURH7BwAA+D9WYzwpKUnbtm3T9OnT5XA4JF04sp2Tk+N5jNvtlsvlkqRSy3NycuRyuVSvXj3l5+eruLhYvr6+pR7/U2JjYxUbG+u5nZ2dXd6bBVQZ7B8AAPx8oaGhZd5n7TSV1NRUvffee5oyZYpq1arlWR4VFaXk5GQVFhYqMzNT6enpat26tVq1aqX09HRlZmaqqKhIycnJioqKksPhUIcOHbR582ZJFwI/KirK1mYBAAAAXnMYY0xFv0h8fLz27dun06dPKyAgQHfddZfWrFmjoqIizwc727Rpoz/84Q+SLpy68vnnn8vHx0ejR49W586dJUnbt2/XihUrVFJSoltvvVV33HGHJOm7775TfHy88vLy1LJlS02YMEE1atTwarYTJ06U+/amTxpb7usEbGjy8hLbIwAA8It3uSPjlRLj1zNiHCgbMQ4AwM93XZ6mAgAAAFR3xDgAAABgCTEOAAAAWEKMAwAAAJYQ4wAAAIAlxDgAAABgCTEOAAAAWEKMAwAAAJYQ4wAAAIAlxDgAAABgCTEOAAAAWEKMAwAAAJYQ4wAAAIAlxDgAAABgCTEOAAAAWEKMAwAAAJYQ4wAAAIAlxDgAAABgCTEOAAAAWEKMAwAAAJYQ4wAAAIAlxDgAAABgCTEOAAAAWEKMAwAAAJYQ4wAAAIAlxDgAAABgCTEOAAAAWEKMAwAAAJYQ4wAAAIAlxDgAAABgCTEOAAAAWEKMAwAAAJYQ4wAAAIAlxDgAAABgCTEOAAAAWEKMAwAAAJYQ4wAAAIAlxDgAAABgCTEOAAAAWEKMAwAAAJYQ4wAAAIAlxDgAAABgCTEOAAAAWEKMAwAAAJYQ4wAAAIAlxDgAAABgCTEOAAAAWEKMAwAAAJYQ4wAAAIAlxDgAAABgCTEOAAAAWEKMAwAAAJYQ4wAAAIAlxDgAAABgCTEOAAAAWEKMAwAAAJYQ4wAAAIAlfpXxIgsXLtT27dsVEBCg2bNnS5Ly8vI0d+5cZWVlqVGjRnr88cfldDpljNGyZcu0Y8cO1apVS+PHj1d4eLgkKSkpSatXr5Yk3XHHHYqJiZEkHTp0SK+++qoKCgrUuXNnjRkzRg6HozI2DQAAALhmlXJkPCYmRk8//XSpZWvXrlXHjh01b948dezYUWvXrpUk7dixQxkZGZo3b57+8Ic/aMmSJZIuxPu7776rmTNnaubMmXr33XeVl5cnSVq8eLEefPBBzZs3TxkZGUpNTa2MzQIAAAB+lkqJ8fbt28vpdJZalpKSon79+kmS+vXrp5SUFEnS1q1b1bdvXzkcDt144406c+aMcnNzlZqaqoiICDmdTjmdTkVERCg1NVW5ubk6e/asbrzxRjkcDvXt29ezLgAAAOB6Zu2c8VOnTqlhw4aSpAYNGujUqVOSJLfbraCgIM/jAgMD5Xa75Xa7FRgY6Fnucrl+cvnFxwMAAADXu0o5Z/xKHA5HpZ3jnZCQoISEBElSXFxcqfAvL+nlvkbAjorYPwAAwP+xFuMBAQHKzc1Vw4YNlZubq/r160u6cMQ7Ozvb87icnBy5XC65XC7t27fPs9ztdqt9+/ZyuVzKycm55PFliY2NVWxsrOf2D18LQGnsHwAA/HyhoaFl3mftNJWoqCitW7dOkrRu3Tp169bNs3z9+vUyxigtLU3+/v5q2LChIiMjtXPnTuXl5SkvL087d+5UZGSkGjZsqDp16igtLU3GGK1fv15RUVG2NgsAAADwWqUcGY+Pj9e+fft0+vRpjRs3TnfddZeGDBmiuXPnKjEx0XNpQ0nq3Lmztm/frkceeUQ1a9bU+PHjJUlOp1NDhw7VU089JUm68847PR8KHTt2rBYuXKiCggJFRkaqc+fOlbFZAAAAwM/iMMYY20PYdOLEiXJfZ/qkseW+TsCGJi8vsT0CAAC/eNflaSoAAABAdUeMAwAAAJYQ4wAAAIAlxDgAAABgCTEOAAAAWHJd/AVOAADwy/XxP/jb06gaBt7epNJfkyPjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCV+tgcAgPIyesUm2yMA5WL5qJ62RwBQSTgyDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFjiZ3uAf/3rX0pMTJTD4VCzZs00fvx4nTx5UvHx8Tp9+rTCw8M1YcIE+fn5qbCwUAsWLNChQ4dUr149PfbYYwoODpYkrVmzRomJifLx8dGYMWMUGRlpd8MAAACAK7B6ZNztduvDDz9UXFycZs+erZKSEiUnJ+uNN97Q4MGDNX/+fNWtW1eJiYmSpMTERNWtW1fz58/X4MGD9eabb0qSjh8/ruTkZM2ZM0fTpk3T0qVLVVJSYnPTAAAAgCuyfppKSUmJCgoKVFxcrIKCAjVo0EB79+5VdHS0JCkmJkYpKSmSpK1btyomJkaSFB0drT179sgYo5SUFPXq1Us1atRQcHCwQkJCdPDgQVubBAAAAHjF6mkqLpdLv/3tb/XQQw+pZs2a6tSpk8LDw+Xv7y9fX1/PY9xut6QLR9IDAwMlSb6+vvL399fp06fldrvVpk2bUuu9+BwAAADgemU1xvPy8pSSkqJXX31V/v7+mjNnjlJTUyv0NRMSEpSQkCBJiouLU1BQULm/Rnq5rxGwoyL2DwBX9svb9/jJh6rBxr5nNcZ3796t4OBg1a9fX5LUo0cP7d+/X/n5+SouLpavr6/cbrdcLpekC0e8c3JyFBgYqOLiYuXn56tevXqe5Rf98Dk/Fhsbq9jYWM/t7OzsCtxC4JeN/QOwg30PsKOi9r3Q0NAy77N6znhQUJAOHDig8+fPyxij3bt3KywsTB06dNDmzZslSUlJSYqKipIkde3aVUlJSZKkzZs3q0OHDnI4HIqKilJycrIKCwuVmZmp9PR0tW7d2tZmAQAAAF6xemS8TZs2io6O1pQpU+Tr66sWLVooNjZWXbp0UXx8vN566y21bNlS/fv3lyT1799fCxYs0IQJE+R0OvXYY49Jkpo1a6aePXvqiSeekI+Pj+6//375+Fj/bCoAAABwWQ5jjLnaJ3333XdyOByea3z/kp04caLc15k+aWy5rxOwocnLS2yPcFVGr9hkewSgXCwf1dP2CFfl439wzjiqhoG3N6mQ9f7s01Ti4+O1f/9+SdLnn3+uJ554QhMnTvRc/xsAAADA1fMqxvfs2aNWrVpJuvAXM5955hnNnDlTa9eurcjZAAAAgCrNq3PGi4qK5OfnJ7fbrby8PLVr106SdOrUqQodDgAAAKjKvIrxFi1aaM2aNcrKylKXLl0kXbh8YJ06dSp0OAAAAKAq8+o0lXHjxunYsWMqKCjQsGHDJElpaWnq06dPhQ4HAAAAVGVeHRkPCQnRo48+WmpZdHS0oqOjK2QoAAAAoDrwKsaNMfrss8+UnJys77//Xq+88or27dunkydPqlevXhU9IwAAAFAleXWayttvv63PP/9cAwYM8PyZ0MDAQL333nsVOhwAAABQlXkV4+vWrdOUKVPUu3dvORwOSVJwcLAyMzMrdDgAAACgKvMqxktKSlS7du1Sy86dO3fJMgAAAADe8yrGO3furL/+9a8qLCyUdOEc8rfffltdu3at0OEAAACAqsyrGL/33nuVm5ur0aNHKz8/X/fee6+ysrJ0zz33VPR8AAAAQJXl1dVU/P39NWnSJJ08eVLZ2dkKCgpSgwYNKng0AAAAoGorM8ZLSkouWVa/fn3Vr1+/1P0+Pl4dXAcAAADwI2XG+N133+3VCt5+++1yGwYAAACoTsqM8QULFlTmHAAAAEC1U2aMN2rU6JJlxhidPn1a9erV81xvHAAAAMC18eoDnGfOnNHrr7+uTZs2qbi4WH5+foqOjtaYMWPkdDorekYAAACgSvLq05cLFy5UQUGBXnrpJf31r3/Viy++qMLCQi1cuLCi5wMAAACqLK9ifM+ePZowYYLCwsJUq1YthYWF6Y9//KP27dtX0fMBAAAAVZZXMd60aVNlZmaWWpadna3Q0NAKGQoAAACoDrw6Z/zmm2/WCy+8oFtuuUVBQUHKzs7Whg0b1LdvXyUmJnoe179//wobFAAAAKhqvIrxAwcOKCQkRAcOHNCBAwckSSEhIUpLS1NaWprnccQ4AAAA4D2vYvzZZ5+t6DkAAACAaserGL8oPz9f586dK7XM5XKV60AAAABAdeFVjO/atUuLFi1SVlbWJfe9/fbb5T4UAAAAUB14FeOvvfaahg4dqt69e6tmzZoVPRMAAABQLXgV44WFhbr11lvl4+PVlRABAAAAeMGruh48eLDee+89GWMqeh4AAACg2vDqyHiPHj30wgsvaO3atapXr16p+xYsWFAhgwEAAABVnVcxPmfOHLVr1049e/bknHEAAACgnHgV45mZmXrxxRc5ZxwAAAAoR17VdVRUlPbs2VPRswAAAADVitdXU3nppZd00003KSAgoNR9Dz/8cIUMBgAAAFR1XsV4s2bN1KxZs4qeBQAAAKhWvIrx3//+9xU9BwAAAFDteBXjklRUVKQTJ07o+++/L7X85ptvLvehAAAAgOrAqxj/6quvNGfOHBUWFurs2bOqU6eOzp07p8DAQK4zDgAAAFwjr66msmLFCt1+++1atmyZ6tSpo2XLlmno0KH61a9+VdHzAQAAAFWWVzF+4sQJDRo0qNSyIUOG6P3336+QoQAAAIDqwKsY9/f319mzZyVJDRo00PHjx5WXl6dz585V6HAAAABAVebVOeM9evTQjh071KdPH91666167rnn5Ovrq+jo6IqeDwAAAKiyvIrx0aNHe76+/fbbdeONN+rs2bPq1KlTRc0FAAAAVHlexfjp06dVr149z+127dpJkjIyMhQSElIxkwEAAABVnFfnjE+cOFE7duwoteyTTz7RtGnTKmQoAAAAoDrw6sj4Qw89pNdee03dunXTb37zG73++uvKzc3V9OnTK3o+AAAAoMry6sh4586dNXv2bH311Vd69NFH5XQ6NWvWLDVv3ryi5wMAAACqLK9i/Ny5c/rrX/+q/Px8DR48WDt27FBSUlIFjwYAAABUbV6dpvLkk0+qbdu2euWVV+Tv76++fftq/vz52rp1q6ZOnVrRMwIAAABVkldHxocPH64JEybI399fktSiRQvNmjVLTZo0qdDhAAAAgKrMqxjv1avXJctq1qypUaNGlftAAAAAQHVx2RifPHlyqdsLFy4sdXvs2LHlPxEAAABQTVw2xjMyMkrdTklJKXW7oKCg/CcCAAAAqonLxrjD4bjsk690PwAAAICyeXXOOAAAAIDyd9lLGxYWFurtt9/23C4oKCh1u6ioqOImAwAAAKq4y8Z4nz59lJOT47ndu3fvS24DAAAAuDaXjfHx48dX+ABnzpzRa6+9pm+++UYOh0MPPfSQQkNDNXfuXGVlZalRo0Z6/PHH5XQ6ZYzRsmXLtGPHDtWqVUvjx49XeHi4JCkpKUmrV6+WJN1xxx2KiYmp8NkBAACAn8Orv8BZkZYtW6bIyEhNnDhRRUVFOn/+vNasWaOOHTtqyJAhWrt2rdauXasRI0Zox44dysjI0Lx583TgwAEtWbJEM2fOVF5ent59913FxcVJkqZOnaqoqCg5nU7LWwcAAACUzeoHOPPz8/Xll1+qf//+kiQ/Pz/VrVtXKSkp6tevnySpX79+nksqbt26VX379pXD4dCNN96oM2fOKDc3V6mpqYqIiJDT6ZTT6VRERIRSU1NtbRYAAADgFatHxjMzM1W/fn0tXLhQR48eVXh4uEaPHq1Tp06pYcOGkqQGDRro1KlTkiS3262goCDP8wMDA+V2u+V2uxUYGOhZ7nK55Ha7K3djAAAAgKtUZoyvXLlSI0eOlCTt2bNHN998c7m/eHFxsQ4fPqz77rtPbdq00bJly7R27dpSj3E4HOV6PfOEhAQlJCRIkuLi4krFfXlJL/c1AnZUxP4B4Mp+efseP/lQNdjY98qM8YSEBE+Mv/zyy1qxYkW5v3hgYKACAwPVpk0bSVJ0dLTWrl2rgIAA5ebmqmHDhsrNzVX9+vUlXTjinZ2d7Xl+Tk6OXC6XXC6X9u3b51nudrvVvn37n3zN2NhYxcbGem7/cH0ASmP/AOxg3wPsqKh9LzQ0tMz7yozxFi1aaPbs2QoLC7vkeuM/9F//9V/XPFiDBg0UGBioEydOKDQ0VLt371ZYWJjCwsK0bt06DRkyROvWrVO3bt0kSVFRUfroo4/Uu3dvHThwQP7+/mrYsKEiIyP1t7/9TXl5eZKknTt3avjw4dc8FwAAAFAZyozxJ554QgkJCcrKypIxptT1xcvTfffdp3nz5qmoqEjBwcEaP368jDGaO3euEhMTPZc2lKTOnTtr+/bteuSRR1SzZk3PpRedTqeGDh2qp556SpJ05513ciUVAAAAXPccxhhzpQctXLiwUq45bsOJEyfKfZ3pk8aW+zoBG5q8vMT2CFdl9IpNtkcAysXyUT1tj3BVPv4H54yjahh4e5MKWe81nabyQ+PHj1deXp62bdsmt9stl8ulrl27cvQZAAAA+Bm8us54WlqaJkyYoE8//VRHjx5VQkKCJkyYoLS0tIqeDwAAAKiyvDoyvnz5co0dO1a9e/f2LEtOTtayZcs0a9asChsOAAAAqMq8OjKenp6unj1Ln78WHR2tjIyMChkKAAAAqA68ivGQkBAlJyeXWrZp0yY1bty4QoYCAAAAqgOvTlMZPXq04uLi9OGHHyooKEhZWVlKT0/X1KlTK3o+AAAAoMryKsbbtm2r+fPna/v27crNzVXXrl3VpUsXrqYCAAAA/Axexbh04Q/r9O3btyJnAQAAAKoVr84ZBwAAAFD+iHEAAADAEmIcAAAAsMTrGM/KyqrIOQAAAIBqx+sYnzx5siTpgw8+qLBhAAAAgOrksldTmTJlisLDw9WyZUuVlJRIkv73f/9XgwYNqpThAAAAgKrsskfGJ06cqE6dOikrK0sFBQWaMmWKioqKtGfPHuXn51fWjAAAAECVdNkYLykpUXR0tO655x7Vrl1bkyZNkjFGH330kSZNmqRHHnmksuYEAAAAqpzLnqYyb948ZWdnKywsTIWFhTpz5oxq1KihJ598UpKUl5dXKUMCAAAAVdFlY3zmzJkqLi7WsWPHNH36dL3++us6d+6cFi9erJYtWyo8PFxOp7OyZgUAAACqlCteTcXX11ctW7aUn5+fnnvuOdWqVUsdOnRQRkaG3nzzzcqYEQAAAKiSLntk/IdGjRolSXI4HOrVq5d69epVYUMBAAAA1YHX1xmPiYmRJM2fP7+iZgEAAACqFa9j/CLOEQcAAADKx1XHOAAAAIDyQYwDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCXEOAAAAGAJMQ4AAABYQowDAAAAlhDjAAAAgCV+tgeQpJKSEk2dOlUul0tTp05VZmam4uPjdfr0aYWHh2vChAny8/NTYWGhFixYoEOHDqlevXp67LHHFBwcLElas2aNEhMT5ePjozFjxigyMtLuRgEAAABXcF0cGf/ggw/UtGlTz+033nhDgwcP1vz581W3bl0lJiZKkhITE1W3bl3Nnz9fgwcP1ptvvilJOn78uJKTkzVnzhxNmzZNS5cuVUlJiZVtAQAAALxlPcZzcnK0fft2DRgwQJJkjNHevXsVHR0tSYqJiVFKSookaevWrYqJiZEkRUdHa8+ePTLGKCUlRb169VKNGjUUHByskJAQHTx40Mr2AAAAAN6yHuPLly/XiBEj5HA4JEmnT5+Wv7+/fH19JUkul0tut1uS5Ha7FRgYKEny9fWVv7+/Tp8+XWr5j58DAAAAXK+snjO+bds2BQQEKDw8XHv37q2U10xISFBCQoIkKS4uTkFBQeX+GunlvkbAjorYPwBc2S9v3+MnH6oGG/ue1Rjfv3+/tm7dqh07dqigoEBnz57V8uXLlZ+fr+LiYvn6+srtdsvlckm6cMQ7JydHgYGBKi4uVn5+vurVq+dZftEPn/NjsbGxio2N9dzOzs6u2I0EfsHYPwA72PcAOypq3wsNDS3zPqunqQwfPlyvvfaaXn31VT322GO6+eab9cgjj6hDhw7avHmzJCkpKUlRUVGSpK5duyopKUmStHnzZnXo0EEOh0NRUVFKTk5WYWGhMjMzlZ6ertatW9vaLAAAAMAr18WlDX/snnvuUXx8vN566y21bNlS/fv3lyT1799fCxYs0IQJE+R0OvXYY49Jkpo1a6aePXvqiSeekI+Pj+6//375+Fg/HR4AAAC4LIcxxtgewqYTJ06U+zrTJ40t93UCNjR5eYntEa7K6BWbbI8AlIvlo3raHuGqfPwPzhlH1TDw9iYVst7r9jQVAAAAoDojxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACwhxgEAAABLiHEAAADAEmIcAAAAsIQYBwAAACzxs/ni2dnZevXVV3Xy5Ek5HA7FxsZq0KBBysvL09y5c5WVlaVGjRrp8ccfl9PplDFGy5Yt044dO1SrVi2NHz9e4eHhkqSkpCStXr1aknTHHXcoJibG4pYBAAAAV2Y1xn19fTVy5EiFh4fr7Nmzmjp1qiIiIpSUlKSOHTtqyJAhWrt2rdauXasRI0Zox44dysjI0Lx583TgwAEtWbJEM2fOVF5ent59913FxcVJkqZOnaqoqCg5nU6bmwcAAABcltXTVBo2bOg5sl2nTh01bdpUbrdbKSkp6tevnySpX79+SklJkSRt3bpVffv2lcPh0I033qgzZ84oNzdXqampioiIkNPplNPpVEREhFJTU21tFgAAAOCV6+ac8czMTB0+fFitW7fWqVOn1LBhQ0lSgwYNdOrUKUmS2+1WUFCQ5zmBgYFyu91yu90KDAz0LHe5XHK73ZW7AQAAAMBVsnqaykXnzp3T7NmzNXr0aPn7+5e6z+FwyOFwlNtrJSQkKCEhQZIUFxdXKu7LS3q5rxGwoyL2DwBX9svb9/jJh6rBxr5nPcaLioo0e/Zs3XLLLerRo4ckKSAgQLm5uWrYsKFyc3NVv359SReOeGdnZ3uem5OTI5fLJZfLpX379nmWu91utW/f/idfLzY2VrGxsZ7bP1wfgNLYPwA72PcAOypq3wsNDS3zPqunqRhj9Nprr6lp06b6zW9+41keFRWldevWSZLWrVunbt26eZavX79exhilpaXJ399fDRs2VGRkpHbu3Km8vDzl5eVp586dioyMtLFJAAAAgNesHhnfv3+/1q9frxtuuEGTJk2SJN19990aMmSI5s6dq8TERM+lDSWpc+fO2r59ux555BHVrFlT48ePlyQ5nU4NHTpUTz31lCTpzjvv5EoqAAAAuO5ZjfF27drpnXfe+cn7pk+ffskyh8OhsWPH/uTj+/fvr/79+5frfAAAAEBFum6upgIAAABUN8Q4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJcQ4AAAAYAkxDgAAAFhCjAMAAACWEOMAAACAJX62ByhPqampWrZsmUpKSjRgwAANGTLE9kgAAABAmarMkfGSkhItXbpUTz/9tObOnauNGzfq+PHjtscCAAAAylRlYvzgwYMKCQlR48aN5efnp169eiklJcX2WAAAAECZqkyMu91uBQYGem4HBgbK7XZbnAgAAAC4vCp1zrg3EhISlJCQIEmKi4tTaGhoub9G6JsflPs6AVzZJ08NtT0CUC2NGVf+P0uB6qLKHBl3uVzKycnx3M7JyZHL5brkcbGxsYqLi1NcXFxljodyNnXqVNsjANUW+x9gB/te1VRlYrxVq1ZKT09XZmamioqKlJycrKioKNtjAQAAAGWqMqep+Pr66r777tMLL7ygkpIS3XrrrWrWrJntsQAAAIAyVZkYl6QuXbqoS5cutsdAJYiNjbU9AlBtsf8BdrDvVU0OY4yxPQQAAABQHVWZc8YBAACAXxpiHL9YmZmZ+uKLL67puSNHjiznaYDq58yZM/r44489t91ut2bPnm1xIqBq+uSTT7Ru3TpJUlJSUqm/o/Laa6/xF8d/4Yhx/GJlZWWVGePFxcWVPA1Q/Zw5c0affPKJ57bL5dLEiRMtTgRUTb/61a/Ur18/SRdiPDc313PfuHHjFBYWZms0lIMq9QFO/DJkZmZq1qxZatu2rdLS0uRyuTR58mS53W4tXbpU33//vWrVqqUHH3xQTZs21auvvqquXbsqOjpa0oWj2itXrtSqVat0/PhxTZo0Sf369ZPT6dS///1vnTt3TiUlJXrqqaf00ksv6cyZMyoqKtKwYcPUrVs3y1sPVJ6r3dcyMjI0f/58nTt3Tt26ddP777+vlStX6ty5cz+5L61atUoZGRmaNGmSIiIiNHDgQL344ouaPXu2pk2bpnHjxnmuajVjxgyNHDlSTZs21euvv65vvvlGxcXF+v3vf89+iSotMzNTM2fOVHh4uA4fPqywsDA9/PDDSktL08qVK1VcXKxWrVrpgQceUI0aNfTmm29q69at8vX1VUREhO6991698847ql27toKDg/X1119r3rx5qlmzpl544QXNnDlTI0eO1Ndff63vvvvO85vfpKQkff3117r//vu1fv16ffjhhyoqKlKbNm00duxY+fhwPPZ6QYzDivT0dD366KMaN26c5syZo82bNyspKUkPPPCAmjRpogMHDmjJkiV69tlny1zH8OHD9c9//tPzRxCSkpJ0+PBhvfLKK3I6nSouLtaTTz4pf39/ff/995o2bZqioqLkcDgqazMB665mX1u+fLl+/etfq0+fPqWOeNeoUeMn96Xhw4frm2++0csvvyzpQnRc1LNnT23atEnNmjVTbm6ucnNz1apVK61atUo333yzxo8frzNnzujpp59Wx44dVbt27Up/b4DKcuLECY0bN07t2rXTwoUL9a9//UsJCQl65plnFBoaqgULFuiTTz5R3759tWXLFsXHx8vhcOjMmTOl1hMdHa2PPvpII0eOVKtWrS65b9q0aZ4YT05O1h133KHjx48rOTlZzz//vPz8/LRkyRJt2LDBc6Qd9hHjsCI4OFgtWrSQJIWHhysrK0v79+/XnDlzPI8pKiq66vVGRETI6XRKkowx+tvf/qYvv/xSDodDbrdbp06dUoMGDcpjE4BfhKvZ19LS0jRp0iRJUp8+fbRy5UpJZe9Ll9OrVy/96U9/0l133aVNmzZ5frO1a9cubdu2Tf/85z8lSQUFBcrOzubX7KjSAgMD1a5dO0lS37599fe//13BwcEKDQ2VJPXr108ff/yxbrvtNtWsWVN//vOf1bVrV3Xt2tXr16hfv74aN26stLQ0NWnSRN9++63atm2rjz/+WIcPH9ZTTz0l6cI+V79+/fLfSFwzYhxW1KhRw/O1j4+PTp06pbp163qOsP2Qr6+vSkpKJEklJSWXjfRatWp5vv7iiy/0/fffKy4uTn5+fvrjH/+ogoKCctwK4Pp3NftaWa5lX3K5XKpXr56OHj2q5ORkPfDAA5IuhP3EiRM9EQJUBz/+jay/v7/y8vIueZyvr69mzpyp3bt3a/Pmzfroo48u+xviH+vVq5c2bdqkpk2bqnv37nI4HDLGqF+/fho+fPjP3g5UDE4YwnWhTp06Cg4O1qZNmyRd+IF95MgRSVKjRo106NAhSdLWrVs9H86sU6eOzp49W+Y68/PzFRAQID8/P+3Zs0dZWVkVuxHAL8Dl9rU2bdro3//+t6QLv+K+qKx96Ur7YM+ePfXee+8pPz9fzZs3lyR16tRJH374oS7+iYvDhw+X+zYC15vs7GylpaVJuvA/t61atVJmZqYyMjIkSevXr1f79u117tw55efnq0uXLho9erSOHj16ybpq165d5n7XvXt3bd26VRs3blTv3r0lSR07dtTmzZs9v83Ky8vj5+F1hiPjuG488sgjWrx4sVavXq2ioiL17t1bLVq00IABA/Tyyy9r0qRJ6tSpk+fo9w033CAfH59SH+D8oT59+ujFF1/UxIkT1apVKzVt2tTGZgHXnbL2tdGjR2v+/PlavXq1IiMj5e/vL6nsfalevXpq27atJk6cqMjISA0cOLDU60RHR2v58uUaOnSoZ9mdd96p5cuX68knn5QxRsHBwZ7PfQBVVWhoqD766CP9+c9/VtOmTTVmzBi1adNGc+bM8XyA8z/+4z+Ul5enl156SYWFhTLG6N57771kXTExMVq8eLHnA5w/5HQ61bRpUx0/flytW7eWJIWFhWnYsGH605/+JGOMfH19df/996tRo0aVsu24Mv4CJwBAknT+/HnVrFlTDodDGzdu1MaNGzV58mTbYwG/aJmZmZ6rDAE/hSPjAABJ0qFDh/T666/LGKO6devqoYcesj0SAFR5HBkHAAAALOEDnAAAAIAlxDgAAABgCTEOAAAAWEKMAwAAAJYQ4wAAAIAlxDgAAABgyf8Hz2+3tEbfH/AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "sns.countplot(x='sentiment', data=df)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('# of Examples')\n",
    "plt.title('# of Examples by Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faced-control",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest Statement:  33 words.\n"
     ]
    }
   ],
   "source": [
    "print('Longest Statement: ', df['num_words'].max(), 'words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "manufactured-creativity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest Statement:  1 words.\n"
     ]
    }
   ],
   "source": [
    "print('Shortest Statement: ', df['num_words'].min(), 'words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-arthritis",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "operating-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 64\n",
    "BATCH_SIZE = 16\n",
    "NUM_CLASSES = 3\n",
    "EPOCHS = 5\n",
    "DROPOUT_PROB = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "NFOLDS = 10\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "grand-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fatal-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tweets, labels, tokenizer, max_length):\n",
    "        self.tweets = tweets\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.tweets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        tweet = str(self.tweets[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            tweet,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True, \n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'  \n",
    "        ) \n",
    "        \n",
    "        return {\n",
    "            'tweet_text': tweet,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "broke-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, tokenizer, max_length):\n",
    "    ds = TweetDataset(tweets=df['text'].to_numpy(),\n",
    "                          labels=df['sentiment'].to_numpy(),\n",
    "                          tokenizer=tokenizer,\n",
    "                          max_length=max_length)\n",
    "    return ds\n",
    "\n",
    "def create_dataloader(ds, batch_size):\n",
    "    return DataLoader(ds, batch_size)#, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "activated-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_ensemble_performance(preds, labels):\n",
    "    preds = np.array(preds)\n",
    "    summed = np.sum(preds, axis=0)\n",
    "    preds = np.argmax(summed, axis=1)\n",
    "    print(confusion_matrix(y_true=labels, y_pred=preds))\n",
    "    print('')\n",
    "    print(classification_report(y_true=labels, y_pred=preds, digits=3, target_names=le.classes_))\n",
    "    \n",
    "def single_model_performance(preds, labels):\n",
    "    print(confusion_matrix(y_true=labels, y_pred=preds))\n",
    "    print('')\n",
    "    print(classification_report(y_true=labels, y_pred=preds, digits=3, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "present-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, data_loader, loss_function, \n",
    "                optimizer, scheduler, n_examples):\n",
    "    \n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_preds = 0\n",
    "    complete_preds = []\n",
    "    complete_labels = []\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        complete_preds.append(preds.data.cpu().numpy().tolist())\n",
    "        complete_labels.append(labels.data.cpu().numpy().tolist())\n",
    "        correct_preds += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    complete_preds_flat = [x for y in complete_preds for x in y]\n",
    "    complete_labels_flat = [x for y in complete_labels for x in y]\n",
    "    acc_score = accuracy_score(y_true=complete_labels_flat, \n",
    "                             y_pred=complete_preds_flat)\n",
    "    return acc_score, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "collected-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, device, data_loader, loss_function, n_examples):\n",
    "    model = model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    correct_preds = 0\n",
    "    complete_preds = []\n",
    "    complete_labels = []\n",
    "    complete_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for item in data_loader:\n",
    "            input_ids = item['input_ids'].to(device)\n",
    "            attention_mask = item['attention_mask'].to(device)\n",
    "            labels = item['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask)\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            \n",
    "            correct_preds += torch.sum(preds == labels)\n",
    "            complete_preds.append(preds.data.cpu().numpy().tolist())\n",
    "            complete_labels.append(labels.data.cpu().numpy().tolist())\n",
    "            complete_outputs.append(outputs.tolist())\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        accuracy = correct_preds.double() / n_examples\n",
    "        complete_preds_flat = [x for y in complete_preds for x in y]\n",
    "        complete_labels_flat = [x for y in complete_labels for x in y]\n",
    "        complete_outputs_flat = [x for y in complete_outputs for x in y]\n",
    "\n",
    "        acc_score = accuracy_score(y_true=complete_labels_flat, \n",
    "                             y_pred=complete_preds_flat)\n",
    "        \n",
    "        return_items = (acc_score, \n",
    "                        np.mean(losses),\n",
    "                        complete_labels_flat,\n",
    "                        complete_preds_flat, \n",
    "                        complete_outputs_flat)\n",
    "        \n",
    "        return return_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "thermal-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(y_test, y_pred, target_names=[-1, 0, 1], \n",
    "            figsize=(5,3)):\n",
    "    \"\"\"Create a labelled confusion matrix plot.\"\"\"\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='BuGn', cbar=False, \n",
    "                ax=ax)\n",
    "    ax.set_title('Confusion matrix')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_xticklabels(target_names)\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_yticklabels(target_names, \n",
    "                       fontdict={'verticalalignment': 'center'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "brief-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pytorch\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "nominated-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(mlf_XP, xp_name_iter, epochs, model, device, train_dataloader, \n",
    "               val_dataloader, test_dataloader, loss_fn, optimizer, \n",
    "               scheduler, model_save_name, n_train, n_val, single_model=True):\n",
    "    \n",
    "    \n",
    "    mlflow.set_experiment(mlf_XP)\n",
    "\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = defaultdict(list)\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        run_name = xp_name_iter + '_' + str(epoch+1)\n",
    "        with mlflow.start_run(run_name = run_name):\n",
    "            epoch_start_time = time.time()\n",
    "            print('Epoch ', epoch+1, '/', epochs)\n",
    "            print('-'*50)\n",
    "\n",
    "            training_output = train_model(model, \n",
    "                                          device, \n",
    "                                          train_dataloader, \n",
    "                                          loss_fn, \n",
    "                                          optimizer, \n",
    "                                          scheduler, \n",
    "                                          n_train)\n",
    "\n",
    "            train_acc, train_loss = training_output\n",
    "\n",
    "            val_output = eval_model(model, \n",
    "                                    device, \n",
    "                                    val_dataloader, \n",
    "                                    loss_fn, \n",
    "                                    n_val)\n",
    "\n",
    "            val_acc, val_loss, _, val_preds, val_outputs = val_output\n",
    "\n",
    "            history['train_accuracy'].append(train_acc)\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_accuracy'].append(val_acc)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_preds'].append(val_preds)\n",
    "            \n",
    "            mlflow.log_metrics({'epoch': epoch, 'train_acc' : train_acc, 'train_loss': train_loss, 'val_acc': val_acc, 'val_loss' : val_loss })\n",
    "            \n",
    "            mlflow.pytorch.log_model(model, run_name, conda_env='/mnt/configs/conda.yml')\n",
    "            \n",
    "            if val_acc > best_accuracy:\n",
    "                torch.save(model.state_dict(), model_save_name)\n",
    "                best_accuracy = val_acc\n",
    "                best_preds = val_preds\n",
    "                best_outputs = val_outputs\n",
    "                best_epoch = epoch\n",
    "                \n",
    "\n",
    "            print('Train Loss: ', \n",
    "                  train_loss, \n",
    "                  ' | ', \n",
    "                  'Train Accuracy: ', \n",
    "                  train_acc)\n",
    "            print('Val Loss: ', \n",
    "                  val_loss, \n",
    "                  ' | ', \n",
    "                  'Val Accuracy: ', \n",
    "                  val_acc)\n",
    "            elapsed_time = format_time(time.time() - epoch_start_time)\n",
    "            print('Epoch Train Time: ', \n",
    "                  elapsed_time)\n",
    "            print('\\n')\n",
    "            \n",
    "            mlflow.set_tag(key=\"elapsed_time\", value=elapsed_time)   \n",
    "\n",
    "    print('Finished Training.')   \n",
    "    print('Fold Train Time: ', format_time(time.time() - start_time))\n",
    "    print('\\n')\n",
    "                  \n",
    "    if single_model:\n",
    "        test_acc, test_loss, test_actuals, test_preds, test_outputs = eval_model(model, \n",
    "                                                    device, \n",
    "                                                    test_dataloader, \n",
    "                                                    loss_function, \n",
    "                                                    len(df_test))\n",
    "\n",
    "        single_model_performance(test_preds, df_test['sentiment'].values)\n",
    "        plot_cm(test_actuals, test_preds)\n",
    "        f1_macro_test = f1_score(test_actuals, test_preds, average='macro')\n",
    "        run_name = xp_name_iter + '_best'\n",
    "        with mlflow.start_run(run_name = run_name):\n",
    "            mlflow.log_metrics({'train_acc' : history['train_accuracy'][epoch], 'train_loss': history['train_accuracy'][epoch], 'val_acc': history['val_accuracy'][epoch], 'val_loss' : history['train_accuracy'][epoch], 'test_acc': test_acc, 'test_loss': test_loss, 'f1_test':f1_macro_test})\n",
    "            mlflow.pytorch.log_model(model, run_name, conda_env='/mnt/configs/conda.yml')\n",
    "\n",
    "                  \n",
    "    return history, best_preds, best_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "blank-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof_and_test_preds(mlf_XP, model_type, tokenizer, \n",
    "                           train_df, test_df, single_model=False):\n",
    "    \n",
    "    \n",
    "    mlflow.set_experiment(mlf_XP)\n",
    "    \n",
    "    \n",
    "    oof_preds = []\n",
    "    oof_outputs = []\n",
    "    oof_preds_indices = []\n",
    "    test_preds_list = []\n",
    "    test_outputs_list = []\n",
    "    history_list = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    fold = 0\n",
    "    \n",
    "    x_train = train_df['text']\n",
    "    y_train = train_df['sentiment']\n",
    "\n",
    "    for train_index, val_index in skf.split(x_train, y_train):\n",
    "        print('Fold: {}'.format(fold+1))\n",
    "        \n",
    "        x_tr = x_train.iloc[train_index]\n",
    "        y_tr = y_train.iloc[train_index]\n",
    "        x_va = x_train.iloc[val_index]\n",
    "        y_va = y_train.iloc[val_index]\n",
    "        \n",
    "        train = pd.DataFrame(list(zip(x_tr, y_tr)), \n",
    "                             columns=['text', 'sentiment'])\n",
    "        val = pd.DataFrame(list(zip(x_va, y_va)), \n",
    "                           columns=['text', 'sentiment'])\n",
    "\n",
    "        train_ds = create_dataset(train, tokenizer, MAX_LENGTH)\n",
    "        val_ds = create_dataset(val, tokenizer, MAX_LENGTH)\n",
    "        test_ds = create_dataset(test_df, tokenizer, MAX_LENGTH)\n",
    "        \n",
    "\n",
    "        if model_type == 'bert':\n",
    "            model = BERTSentimentClassifier(NUM_CLASSES)\n",
    "            model = model.to(device)\n",
    "        elif model_type == 'distilbert':\n",
    "            model = DistilBertForSequenceClassification(pretrained_model_name=DISTILBERT_MODEL_NAME, \n",
    "                                                        num_classes=NUM_CLASSES)\n",
    "            model = model.to(device)\n",
    "        elif model_type == 'roberta':\n",
    "            model = RobertaSentimentClassifier(n_classes=NUM_CLASSES)\n",
    "            model = model.to(device)\n",
    "        \n",
    "        train_loader = create_dataloader(train_ds, BATCH_SIZE)\n",
    "        val_loader = create_dataloader(val_ds, BATCH_SIZE)\n",
    "        test_loader = create_dataloader(test_ds, BATCH_SIZE)\n",
    "        \n",
    "        training_steps = len(train_loader.dataset) * EPOCHS\n",
    "        warmup_steps = int(0.1 * training_steps)\n",
    "        optimizer = AdamW(model.parameters(), \n",
    "                          lr=LEARNING_RATE, \n",
    "                          weight_decay=WEIGHT_DECAY, \n",
    "                          correct_bias=True)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps=warmup_steps, \n",
    "                                                    num_training_steps=training_steps)\n",
    "        \n",
    "        model_save_name = '{}_fold_{}.bin'.format(model_type, fold)\n",
    "        \n",
    "        history, preds, outputs = train_fold(mlf_XP = mlf_XP, \n",
    "                                             xp_name_iter = model_type + '_Fold' + str(fold+1),\n",
    "                                             epochs=EPOCHS,\n",
    "                                             model=model, \n",
    "                                             device=device, \n",
    "                                             train_dataloader=train_loader, \n",
    "                                             val_dataloader=val_loader,\n",
    "                                             test_dataloader=test_loader,\n",
    "                                             loss_fn=loss_function,\n",
    "                                             optimizer=optimizer,\n",
    "                                             scheduler=scheduler,\n",
    "                                             model_save_name=model_save_name,\n",
    "                                             n_train=len(train),\n",
    "                                             n_val=len(val),\n",
    "                                             single_model=False\n",
    "                                            )\n",
    "        \n",
    "        history_list.append(history)\n",
    "        oof_preds.append(preds)\n",
    "        oof_outputs.append(outputs)\n",
    "        oof_preds_indices.append(val_index)\n",
    "        \n",
    "        test_acc, test_loss, test_actuals, test_preds, test_outputs = eval_model(model, \n",
    "                                                                                device, \n",
    "                                                                                test_loader, \n",
    "                                                                                loss_function, \n",
    "                                                                                len(test_df))\n",
    "        test_preds_list.append(test_preds)\n",
    "        test_outputs_list.append(test_outputs)\n",
    "        \n",
    "        fold += 1\n",
    "\n",
    "    print(str(NFOLDS), 'Fold CV Train Time: ', format_time(time.time() - start_time))\n",
    "\n",
    "    return history_list, test_outputs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "square-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "roman-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-shirt",
   "metadata": {},
   "source": [
    "## Données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-latitude",
   "metadata": {},
   "source": [
    ":::{warning}\n",
    "On est obligé de recoder les sorties car laisser un label cible négatif génère une erreur `CUDA error: device-side assert triggered`\n",
    "cf [lien](https://discuss.pytorch.org/t/runtimeerror-cuda-error-device-side-assert-triggered/34213/8)\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "documentary-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df_train['sentiment'] = le.fit_transform(df_train['sentiment'])\n",
    "df_val['sentiment'] = le.fit_transform(df_val['sentiment'])\n",
    "df_test['sentiment'] = le.fit_transform(df_test['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "relevant-robertson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21979</th>\n",
       "      <td>No allowed a calculator for this exam despite ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21980</th>\n",
       "      <td>Haha same as miine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21981</th>\n",
       "      <td>i`m sorry people are so rude to you, isaac, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21982</th>\n",
       "      <td>why? i enjoy fancy meals on my own smtimes, t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21983</th>\n",
       "      <td>oh yeah - love his choregoraphy. the pants......</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21984 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "0                    I`d have responded, if I were going          1\n",
       "1          Sooo SAD I will miss you here in San Diego!!!          0\n",
       "2                              my boss is bullying me...          0\n",
       "3                         what interview! leave me alone          0\n",
       "4       Sons of ****, why couldn`t they put them on t...          0\n",
       "...                                                  ...        ...\n",
       "21979  No allowed a calculator for this exam despite ...          0\n",
       "21980                                 Haha same as miine          1\n",
       "21981    i`m sorry people are so rude to you, isaac, ...          0\n",
       "21982   why? i enjoy fancy meals on my own smtimes, t...          2\n",
       "21983   oh yeah - love his choregoraphy. the pants......          1\n",
       "\n",
       "[21984 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "discrete-republican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21984</th>\n",
       "      <td>_JessicaB_**** yip.....aw gonna miss them on bb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21985</th>\n",
       "      <td>_violence heyyyy babyy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21986</th>\n",
       "      <td>Up at 6am on Sunday... Going to meet my mom fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21987</th>\n",
       "      <td>so the Today show still hasn`t gotten in touch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21988</th>\n",
       "      <td>Just checked email and got a follower withb sa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27475</th>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5496 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "21984    _JessicaB_**** yip.....aw gonna miss them on bb          0\n",
       "21985                             _violence heyyyy babyy          0\n",
       "21986  Up at 6am on Sunday... Going to meet my mom fo...          1\n",
       "21987  so the Today show still hasn`t gotten in touch...          1\n",
       "21988  Just checked email and got a follower withb sa...          1\n",
       "...                                                  ...        ...\n",
       "27475   wish we could come see u on Denver  husband l...          0\n",
       "27476   I`ve wondered about rake to.  The client has ...          0\n",
       "27477   Yay good for both of you. Enjoy the break - y...          2\n",
       "27478                         But it was worth it  ****.          2\n",
       "27479     All this flirting going on - The ATG smiles...          1\n",
       "\n",
       "[5496 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "usual-liver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27475</th>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27480 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "0                    I`d have responded, if I were going          1\n",
       "1          Sooo SAD I will miss you here in San Diego!!!          0\n",
       "2                              my boss is bullying me...          0\n",
       "3                         what interview! leave me alone          0\n",
       "4       Sons of ****, why couldn`t they put them on t...          0\n",
       "...                                                  ...        ...\n",
       "27475   wish we could come see u on Denver  husband l...          0\n",
       "27476   I`ve wondered about rake to.  The client has ...          0\n",
       "27477   Yay good for both of you. Enjoy the break - y...          2\n",
       "27478                         But it was worth it  ****.          2\n",
       "27479     All this flirting going on - The ATG smiles...          1\n",
       "\n",
       "[27480 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_full = pd.concat([df_train, df_val])\n",
    "df_train_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-sigma",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-grenada",
   "metadata": {},
   "source": [
    "### Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "retired-register",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550faa3de4784867ab253cdc8cc61a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e0a489fffb4aa48a03ae5f7f35be25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3f4421a41e454daae10f6c16be8fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DISTILBERT_MODEL_NAME = 'distilbert-base-uncased'\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(DISTILBERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "hindu-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_train_ds = create_dataset(df_train, distilbert_tokenizer, MAX_LENGTH)\n",
    "distilbert_test_ds = create_dataset(df_test, distilbert_tokenizer, MAX_LENGTH)\n",
    "distilbert_val_ds = create_dataset(df_val, distilbert_tokenizer, MAX_LENGTH)\n",
    "\n",
    "distilbert_train_dataloader = create_dataloader(distilbert_train_ds, BATCH_SIZE)\n",
    "distilbert_test_dataloader = create_dataloader(distilbert_test_ds, BATCH_SIZE)\n",
    "distilbert_val_dataloader = create_dataloader(distilbert_val_ds, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "formed-dakota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TweetDataset at 0x7f1ee40c2fa0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "subtle-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertForSequenceClassification(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_model_name, num_classes):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(pretrained_model_name, num_labels=num_classes)\n",
    "\n",
    "        self.distilbert = AutoModel.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        self.classifier = nn.Linear(config.dim, num_classes)\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, head_mask=None):\n",
    "\n",
    "        assert attention_mask is not None, \"No Attention Mask\"\n",
    "        distilbert_output = self.distilbert(input_ids=input_ids,\n",
    "                                            attention_mask=attention_mask,\n",
    "                                            head_mask=head_mask)\n",
    "\n",
    "        hidden_state = distilbert_output[0]  \n",
    "        pooled_output = hidden_state[:, 0] \n",
    "        pooled_output = self.pre_classifier(pooled_output)  \n",
    "        pooled_output = nn.ReLU()(pooled_output)  \n",
    "        pooled_output = self.dropout(pooled_output)  \n",
    "        logits = self.classifier(pooled_output)  \n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "union-actress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e63308e0f8414e9ffe4e181801bfdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867db49b696948a1aa7ceddad4db9873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distilbert_model = DistilBertForSequenceClassification(pretrained_model_name=DISTILBERT_MODEL_NAME, num_classes=NUM_CLASSES)\n",
    "distilbert_model = distilbert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "charitable-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = len(distilbert_train_dataloader.dataset) * EPOCHS\n",
    "\n",
    "distilbert_optimizer = AdamW(distilbert_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, correct_bias=True)\n",
    "distilbert_scheduler = get_linear_schedule_with_warmup(distilbert_optimizer, num_warmup_steps=int(0.1 * training_steps), num_training_steps=training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "instructional-reform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "failing-disclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 / 5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-b148a0014def>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m distilbert_history, distilbert_preds, distilbert_outputs = train_fold(mlf_XP='BERT', \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                                        \u001b[0mxp_name_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'DistilBERT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                        \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistilbert_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                        \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-da653fe3e061>\u001b[0m in \u001b[0;36mtrain_fold\u001b[0;34m(mlf_XP, xp_name_iter, epochs, model, device, train_dataloader, val_dataloader, test_dataloader, loss_fn, optimizer, scheduler, model_save_name, n_train, n_val, single_model)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             training_output = train_model(model, \n\u001b[0m\u001b[1;32m     22\u001b[0m                                           \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                           \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-8a0218311ad4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, device, data_loader, loss_function, optimizer, scheduler, n_examples)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "distilbert_history, distilbert_preds, distilbert_outputs = train_fold(mlf_XP='BERT', \n",
    "                                                                       xp_name_iter='DistilBERT',\n",
    "                                                                       epochs=EPOCHS,\n",
    "                                                                       model=distilbert_model,\n",
    "                                                                       device=device, \n",
    "                                                                       train_dataloader=distilbert_train_dataloader, \n",
    "                                                                       val_dataloader=distilbert_val_dataloader,\n",
    "                                                                       test_dataloader=distilbert_test_dataloader,\n",
    "                                                                       loss_fn=loss_function,\n",
    "                                                                       optimizer=distilbert_optimizer,\n",
    "                                                                       scheduler=distilbert_scheduler,\n",
    "                                                                       model_save_name='distilbest_best_model.bin',\n",
    "                                                                       n_train=len(df_train),\n",
    "                                                                       n_val=len(df_val),\n",
    "                                                                       single_model=True\n",
    "                                                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "egyptian-beaver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(distilbert_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "native-junction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5496"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(distilbert_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "transsexual-examination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5496"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(distilbert_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "alpha-player",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy bday!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>its at 3 am, im very tired but i can`t sleep  ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>All alone in this old house again.  Thanks for...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>I know what you mean. My little dog is sinkin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>_sutra what is your next youtube video gonna b...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>http://twitpic.com/4woj2 - omgssh  ang cute n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3534 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  sentiment\n",
       "0     Last session of the day  http://twitpic.com/67ezh          1\n",
       "1      Shanghai is also really exciting (precisely -...          2\n",
       "2     Recession hit Veronique Branquinho, she has to...          0\n",
       "3                                           happy bday!          2\n",
       "4                http://twitpic.com/4w75p - I like it!!          2\n",
       "...                                                 ...        ...\n",
       "3529  its at 3 am, im very tired but i can`t sleep  ...          0\n",
       "3530  All alone in this old house again.  Thanks for...          2\n",
       "3531   I know what you mean. My little dog is sinkin...          0\n",
       "3532  _sutra what is your next youtube video gonna b...          2\n",
       "3533   http://twitpic.com/4woj2 - omgssh  ang cute n...          2\n",
       "\n",
       "[3534 rows x 2 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "comparable-ensemble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27475</th>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27480 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "0                    I`d have responded, if I were going          1\n",
       "1          Sooo SAD I will miss you here in San Diego!!!          0\n",
       "2                              my boss is bullying me...          0\n",
       "3                         what interview! leave me alone          0\n",
       "4       Sons of ****, why couldn`t they put them on t...          0\n",
       "...                                                  ...        ...\n",
       "27475   wish we could come see u on Denver  husband l...          0\n",
       "27476   I`ve wondered about rake to.  The client has ...          0\n",
       "27477   Yay good for both of you. Enjoy the break - y...          2\n",
       "27478                         But it was worth it  ****.          2\n",
       "27479     All this flirting going on - The ATG smiles...          1\n",
       "\n",
       "[27480 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-hundred",
   "metadata": {},
   "source": [
    "### 10-Fold CV¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "split-bacteria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Epoch  1 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.941926259658315  |  Train Accuracy:  0.522400129387029\n",
      "Val Loss:  0.6203636002055434  |  Val Accuracy:  0.7558224163027657\n",
      "Epoch Train Time:  0:02:25\n",
      "\n",
      "\n",
      "Epoch  2 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.580964529094073  |  Train Accuracy:  0.7636665049328805\n",
      "Val Loss:  0.5477447283649167  |  Val Accuracy:  0.7776564774381368\n",
      "Epoch Train Time:  0:02:18\n",
      "\n",
      "\n",
      "Epoch  3 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5048014043286964  |  Train Accuracy:  0.7982371017305515\n",
      "Val Loss:  0.545227823259179  |  Val Accuracy:  0.7900291120815138\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  4 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.44333723517084184  |  Train Accuracy:  0.8298964903768398\n",
      "Val Loss:  0.5853824304893266  |  Val Accuracy:  0.7863901018922853\n",
      "Epoch Train Time:  0:02:15\n",
      "\n",
      "\n",
      "Epoch  5 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.3777913040834519  |  Train Accuracy:  0.8617176128093159\n",
      "Val Loss:  0.6538275292273178  |  Val Accuracy:  0.784570596797671\n",
      "Epoch Train Time:  0:02:15\n",
      "\n",
      "\n",
      "Finished Training.\n",
      "Fold Train Time:  0:11:30\n",
      "\n",
      "\n",
      "Fold: 2\n",
      "Epoch  1 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.9205003487377549  |  Train Accuracy:  0.5397865114022319\n",
      "Val Loss:  0.6180462270628574  |  Val Accuracy:  0.7456331877729258\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  2 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.579977193917178  |  Train Accuracy:  0.7666181465308103\n",
      "Val Loss:  0.5480122222716726  |  Val Accuracy:  0.7762008733624454\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  3 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5064557375046599  |  Train Accuracy:  0.7976306000323468\n",
      "Val Loss:  0.5441750175384588  |  Val Accuracy:  0.7878457059679768\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  4 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.44550948646425587  |  Train Accuracy:  0.8284813197476953\n",
      "Val Loss:  0.5777701539702194  |  Val Accuracy:  0.7918486171761281\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  5 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.3740355369091554  |  Train Accuracy:  0.8634562510108361\n",
      "Val Loss:  0.6463065561188688  |  Val Accuracy:  0.787117903930131\n",
      "Epoch Train Time:  0:02:15\n",
      "\n",
      "\n",
      "Finished Training.\n",
      "Fold Train Time:  0:11:22\n",
      "\n",
      "\n",
      "Fold: 3\n",
      "Epoch  1 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.9184577913827143  |  Train Accuracy:  0.5359453339802684\n",
      "Val Loss:  0.6066395441113517  |  Val Accuracy:  0.7554585152838428\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  2 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5813285425054765  |  Train Accuracy:  0.7635047711466926\n",
      "Val Loss:  0.5406240603431712  |  Val Accuracy:  0.7852983988355168\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  3 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5067589409977082  |  Train Accuracy:  0.7990862041080382\n",
      "Val Loss:  0.5343165016451548  |  Val Accuracy:  0.7914847161572053\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  4 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.4455857805083632  |  Train Accuracy:  0.8299773572699337\n",
      "Val Loss:  0.5721239838315997  |  Val Accuracy:  0.7918486171761281\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  5 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.3778618352793707  |  Train Accuracy:  0.8634562510108361\n",
      "Val Loss:  0.6522320938015054  |  Val Accuracy:  0.7885735080058224\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Finished Training.\n",
      "Fold Train Time:  0:11:22\n",
      "\n",
      "\n",
      "Fold: 4\n",
      "Epoch  1 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.9359287480988755  |  Train Accuracy:  0.5133834708070516\n",
      "Val Loss:  0.6244141412682311  |  Val Accuracy:  0.745269286754003\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  2 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5799717282416132  |  Train Accuracy:  0.7639091056121624\n",
      "Val Loss:  0.5602689084358686  |  Val Accuracy:  0.774745269286754\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  3 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.505196814004809  |  Train Accuracy:  0.7991670710011322\n",
      "Val Loss:  0.5649052176971076  |  Val Accuracy:  0.7791120815138283\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  4 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.4425473887989006  |  Train Accuracy:  0.8301795245026686\n",
      "Val Loss:  0.6196258552805629  |  Val Accuracy:  0.7834788937409025\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  5 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.37172359224399226  |  Train Accuracy:  0.864022319262494\n",
      "Val Loss:  0.7073620971465526  |  Val Accuracy:  0.774745269286754\n",
      "Epoch Train Time:  0:02:15\n",
      "\n",
      "\n",
      "Finished Training.\n",
      "Fold Train Time:  0:11:22\n",
      "\n",
      "\n",
      "Fold: 5\n",
      "Epoch  1 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.9415115041689608  |  Train Accuracy:  0.5170629144428271\n",
      "Val Loss:  0.6350645698433699  |  Val Accuracy:  0.7434497816593887\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  2 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5827579239014666  |  Train Accuracy:  0.7607957302280446\n",
      "Val Loss:  0.5471963639869246  |  Val Accuracy:  0.7867540029112081\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  3 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5057246983581419  |  Train Accuracy:  0.7993692382338671\n",
      "Val Loss:  0.5351174239848935  |  Val Accuracy:  0.7998544395924309\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  4 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.4470801455044654  |  Train Accuracy:  0.8266213812065341\n",
      "Val Loss:  0.5497014267413423  |  Val Accuracy:  0.7991266375545851\n",
      "Epoch Train Time:  0:02:15\n",
      "\n",
      "\n",
      "Epoch  5 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.37871522154962645  |  Train Accuracy:  0.86139414523694\n",
      "Val Loss:  0.6404869829759349  |  Val Accuracy:  0.7962154294032023\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Finished Training.\n",
      "Fold Train Time:  0:11:21\n",
      "\n",
      "\n",
      "Fold: 6\n",
      "Epoch  1 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.9295003596129423  |  Train Accuracy:  0.5452045932395277\n",
      "Val Loss:  0.6083728503002677  |  Val Accuracy:  0.75509461426492\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  2 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5811521254605186  |  Train Accuracy:  0.7621300339640951\n",
      "Val Loss:  0.5365904703909574  |  Val Accuracy:  0.784570596797671\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  3 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5050301146952563  |  Train Accuracy:  0.7982775351770985\n",
      "Val Loss:  0.5421336143342561  |  Val Accuracy:  0.7867540029112081\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  4 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.4430553875285241  |  Train Accuracy:  0.8300582241630277\n",
      "Val Loss:  0.5596038565732712  |  Val Accuracy:  0.7958515283842795\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  5 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.3792365301860663  |  Train Accuracy:  0.8621623807213327\n",
      "Val Loss:  0.634867753075479  |  Val Accuracy:  0.7893013100436681\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Finished Training.\n",
      "Fold Train Time:  0:11:22\n",
      "\n",
      "\n",
      "Fold: 7\n",
      "Epoch  1 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.9052738494313242  |  Train Accuracy:  0.5478732007116287\n",
      "Val Loss:  0.6165331214839636  |  Val Accuracy:  0.7536390101892285\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  2 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5792397190401039  |  Train Accuracy:  0.7655264434740418\n",
      "Val Loss:  0.5519887229730917  |  Val Accuracy:  0.7812954876273653\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  3 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.503106549497872  |  Train Accuracy:  0.7999757399320718\n",
      "Val Loss:  0.5589800931859848  |  Val Accuracy:  0.7842066957787481\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  4 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.4417677537561003  |  Train Accuracy:  0.8299773572699337\n",
      "Val Loss:  0.5645578062205121  |  Val Accuracy:  0.7947598253275109\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  5 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.3713598938401003  |  Train Accuracy:  0.8643457868348698\n",
      "Val Loss:  0.6417522168783254  |  Val Accuracy:  0.7903930131004366\n",
      "Epoch Train Time:  0:02:15\n",
      "\n",
      "\n",
      "Finished Training.\n",
      "Fold Train Time:  0:11:21\n",
      "\n",
      "\n",
      "Fold: 8\n",
      "Epoch  1 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.9064137189423745  |  Train Accuracy:  0.56420831311661\n",
      "Val Loss:  0.60685450930235  |  Val Accuracy:  0.75254730713246\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  2 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5819097766255114  |  Train Accuracy:  0.7641517062914442\n",
      "Val Loss:  0.531711004327896  |  Val Accuracy:  0.7812954876273653\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  3 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5045987542087326  |  Train Accuracy:  0.7986818696425684\n",
      "Val Loss:  0.5272369070281816  |  Val Accuracy:  0.7918486171761281\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  4 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.44432073352041557  |  Train Accuracy:  0.8300986576095747\n",
      "Val Loss:  0.5473811943084002  |  Val Accuracy:  0.7940320232896652\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  5 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.3745883988724952  |  Train Accuracy:  0.8643457868348698\n",
      "Val Loss:  0.6393082001094901  |  Val Accuracy:  0.7831149927219796\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Finished Training.\n",
      "Fold Train Time:  0:11:22\n",
      "\n",
      "\n",
      "Fold: 9\n",
      "Epoch  1 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.9189361214522094  |  Train Accuracy:  0.5487223030891153\n",
      "Val Loss:  0.6407536754254685  |  Val Accuracy:  0.7245269286754003\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  2 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5772791211655963  |  Train Accuracy:  0.7660520782791526\n",
      "Val Loss:  0.582675036476102  |  Val Accuracy:  0.7612809315866085\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  3 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5016376986608083  |  Train Accuracy:  0.8010674429888404\n",
      "Val Loss:  0.6005498271782038  |  Val Accuracy:  0.7711062590975255\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  4 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.43999696849697156  |  Train Accuracy:  0.8320394630438298\n",
      "Val Loss:  0.6376301472963288  |  Val Accuracy:  0.7641921397379913\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  5 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.3717861590728665  |  Train Accuracy:  0.8649522885330746\n",
      "Val Loss:  0.7558341780894025  |  Val Accuracy:  0.7634643377001455\n",
      "Epoch Train Time:  0:02:15\n",
      "\n",
      "\n",
      "Finished Training.\n",
      "Fold Train Time:  0:11:20\n",
      "\n",
      "\n",
      "Fold: 10\n",
      "Epoch  1 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.9245070222124245  |  Train Accuracy:  0.5524826136179848\n",
      "Val Loss:  0.6224677699596383  |  Val Accuracy:  0.74745269286754\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  2 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5746363384160003  |  Train Accuracy:  0.7660520782791526\n",
      "Val Loss:  0.545842379764762  |  Val Accuracy:  0.777292576419214\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  3 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.5028234107364427  |  Train Accuracy:  0.8005013747371826\n",
      "Val Loss:  0.5520749990503455  |  Val Accuracy:  0.784570596797671\n",
      "Epoch Train Time:  0:02:17\n",
      "\n",
      "\n",
      "Epoch  4 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.44011698519861636  |  Train Accuracy:  0.8330502992075044\n",
      "Val Loss:  0.5861991313394419  |  Val Accuracy:  0.7885735080058224\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Epoch  5 / 5\n",
      "--------------------------------------------------\n",
      "Train Loss:  0.3778763751934293  |  Train Accuracy:  0.8624454148471615\n",
      "Val Loss:  0.6675134595093686  |  Val Accuracy:  0.7816593886462883\n",
      "Epoch Train Time:  0:02:16\n",
      "\n",
      "\n",
      "Finished Training.\n",
      "Fold Train Time:  0:11:22\n",
      "\n",
      "\n",
      "10 Fold CV Train Time:  1:54:57\n"
     ]
    }
   ],
   "source": [
    "distilbert_history, distilbert_test_outputs = get_oof_and_test_preds(mlf_XP='DistilBERT_10Fold',\n",
    "                                                                     model_type='distilbert', \n",
    "                                                                     tokenizer=distilbert_tokenizer, \n",
    "                                                                     train_df=df_train_full, \n",
    "                                                                     test_df=df_test,\n",
    "                                                                     single_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "plain-portal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 739  235   27]\n",
      " [ 148 1123  159]\n",
      " [  20  165  918]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.815     0.738     0.775      1001\n",
      "     neutral      0.737     0.785     0.761      1430\n",
      "    positive      0.832     0.832     0.832      1103\n",
      "\n",
      "    accuracy                          0.787      3534\n",
      "   macro avg      0.795     0.785     0.789      3534\n",
      "weighted avg      0.789     0.787     0.787      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_ensemble_performance(distilbert_test_outputs, df_test['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "mental-north",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21979</th>\n",
       "      <td>No allowed a calculator for this exam despite ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21980</th>\n",
       "      <td>Haha same as miine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21981</th>\n",
       "      <td>i`m sorry people are so rude to you, isaac, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21982</th>\n",
       "      <td>why? i enjoy fancy meals on my own smtimes, t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21983</th>\n",
       "      <td>oh yeah - love his choregoraphy. the pants......</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21984 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "0                    I`d have responded, if I were going          1\n",
       "1          Sooo SAD I will miss you here in San Diego!!!          0\n",
       "2                              my boss is bullying me...          0\n",
       "3                         what interview! leave me alone          0\n",
       "4       Sons of ****, why couldn`t they put them on t...          0\n",
       "...                                                  ...        ...\n",
       "21979  No allowed a calculator for this exam despite ...          0\n",
       "21980                                 Haha same as miine          1\n",
       "21981    i`m sorry people are so rude to you, isaac, ...          0\n",
       "21982   why? i enjoy fancy meals on my own smtimes, t...          2\n",
       "21983   oh yeah - love his choregoraphy. the pants......          1\n",
       "\n",
       "[21984 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "micro-january",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text\n",
       "sentiment      \n",
       "0          6195\n",
       "1          8849\n",
       "2          6940"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "promotional-modification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text\n",
       "sentiment      \n",
       "0          1001\n",
       "1          1430\n",
       "2          1103"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-bibliography",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "blond-divide",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-accused",
   "metadata": {},
   "source": [
    "### Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "documented-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'bert-base-cased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "experimental-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_ds = create_dataset(df_train, bert_tokenizer, MAX_LENGTH)\n",
    "bert_test_ds = create_dataset(df_test, bert_tokenizer, MAX_LENGTH)\n",
    "bert_val_ds = create_dataset(df_val, bert_tokenizer, MAX_LENGTH)\n",
    "\n",
    "bert_train_dataloader = create_dataloader(bert_train_ds, BATCH_SIZE)\n",
    "bert_test_dataloader = create_dataloader(bert_test_ds, BATCH_SIZE)\n",
    "bert_val_dataloader = create_dataloader(bert_val_ds, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fixed-billion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet_text': [' I`d have responded, if I were going', ' Sooo SAD I will miss you here in San Diego!!!', 'my boss is bullying me...', ' what interview! leave me alone', ' Sons of ****, why couldn`t they put them on the releases we already bought', 'http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth', '2am feedings for the baby are fun when he is all smiles and coos', 'Soooo high', ' Both of you', ' Journey!? Wow... u just became cooler.  hehe... (is that possible!?)', ' as much as i love to be hopeful, i reckon the chances are minimal =P i`m never gonna get my cake and stuff', 'I really really like the song Love Story by Taylor Swift', 'My Sharpie is running DANGERously low on ink', 'i want to go to music tonight but i lost my voice.', 'test test from the LG enV2', 'Uh oh, I am sunburned'], 'input_ids': tensor([[  101,   146,   169,  ...,     0,     0,     0],\n",
      "        [  101, 27972,  1186,  ...,     0,     0,     0],\n",
      "        [  101,  1139,  6054,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,   178,  1328,  ...,     0,     0,     0],\n",
      "        [  101,  2774,  2774,  ...,     0,     0,     0],\n",
      "        [  101, 11205,  9294,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 0, 0, 0, 0, 1, 2, 1, 1, 2, 1, 2, 0, 0, 1, 0])}\n",
      "\n",
      "---------------------\n",
      "\n",
      "{'tweet_text': [' S`ok, trying to plot alternatives as we speak *sigh*', 'i`ve been sick for the past few days  and thus, my hair looks wierd.  if i didnt have a hat on it would look... http://tinyurl.com/mnf4kw', 'is back home now      gonna miss every one', 'Hes just not that into you', ' oh Marly, I`m so sorry!!  I hope you find her soon!! <3 <3', 'Playing Ghost Online is really interesting. The new updates are Kirin pet and Metamorph for third job.  Can`t wait to have a dragon pet', 'is cleaning the house for her family who is comming later today..', 'gotta restart my computer .. I thought Win7 was supposed to put an end to the constant rebootiness', 'SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cALLed LoSe f0LloWeRs FridAy... smH', 'the free fillin` app on my ipod is fun, im addicted', '  I`m sorry.', 'On the way to Malaysia...no internet access to Twit', 'juss came backk from Berkeleyy ; omg its madd fun out there  havent been out there in a minute . whassqoodd ?', 'Went to sleep and there is a power cut in Noida  Power back up not working too', 'I`m going home now. Have you seen my new twitter design? Quite....heavenly isn`****?', 'i hope unni will make the audition . fighting dahye unni !'], 'input_ids': tensor([[  101,   156,   169,  ...,     0,     0,     0],\n",
      "        [  101,   178,   169,  ...,     0,     0,     0],\n",
      "        [  101,  1110,  1171,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 23158,  1204,  ...,     0,     0,     0],\n",
      "        [  101,   146,   169,  ...,     0,     0,     0],\n",
      "        [  101,   178,  2810,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 0, 0, 1, 1, 2, 1, 1, 1, 2, 0, 0, 2, 0, 2, 2])}\n",
      "\n",
      "---------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for batch in bert_train_dataloader:\n",
    "    i+=1\n",
    "    if i<3:\n",
    "        print(batch)\n",
    "        print('\\n---------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "military-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(BERTSentimentClassifier, self).__init__()\n",
    "        self.model = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(DROPOUT_PROB)\n",
    "        self.output = nn.Linear(self.model.config.hidden_size, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        output = self.drop(pooled_output)\n",
    "        \n",
    "        return self.output(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "isolated-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERTSentimentClassifier(NUM_CLASSES)\n",
    "bert_model = bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "alone-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = len(bert_train_dataloader.dataset) * EPOCHS\n",
    "\n",
    "bert_optimizer = AdamW(bert_model.parameters(), \n",
    "                       lr=LEARNING_RATE, \n",
    "                       weight_decay=WEIGHT_DECAY, \n",
    "                       correct_bias=True)\n",
    "\n",
    "warmup_steps = int(0.1 * training_steps)\n",
    "bert_scheduler = get_linear_schedule_with_warmup(bert_optimizer, \n",
    "                                                 num_warmup_steps=warmup_steps, \n",
    "                                                 num_training_steps=training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "electric-favor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "unable-batch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTSentimentClassifier(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (output): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "altered-timeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 / 5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dropout(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-9eb5e96f7792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m bert_single_model_items = train_fold(mlf_XP='BERT', \n\u001b[0m\u001b[1;32m      2\u001b[0m                                      \u001b[0mxp_name_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BERT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                      \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                      \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                      \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-da653fe3e061>\u001b[0m in \u001b[0;36mtrain_fold\u001b[0;34m(mlf_XP, xp_name_iter, epochs, model, device, train_dataloader, val_dataloader, test_dataloader, loss_fn, optimizer, scheduler, model_save_name, n_train, n_val, single_model)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             training_output = train_model(model, \n\u001b[0m\u001b[1;32m     22\u001b[0m                                           \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                           \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-8a0218311ad4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, device, data_loader, loss_function, optimizer, scheduler, n_examples)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-eee84f7ea966>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m         )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: dropout(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "bert_single_model_items = train_fold(mlf_XP='BERT', \n",
    "                                     xp_name_iter='BERT',\n",
    "                                     epochs=EPOCHS, \n",
    "                                     model=bert_model, \n",
    "                                     device=device, \n",
    "                                     train_dataloader=bert_train_dataloader, \n",
    "                                     val_dataloader=bert_val_dataloader,\n",
    "                                     test_dataloader=bert_test_dataloader,\n",
    "                                     loss_fn=loss_function,\n",
    "                                     optimizer=bert_optimizer,\n",
    "                                     scheduler=bert_scheduler,\n",
    "                                     model_save_name='bert_best_model.bin',\n",
    "                                     n_train=len(df_train),\n",
    "                                     n_val=len(df_val),\n",
    "                                     single_model=True\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-beijing",
   "metadata": {},
   "source": [
    "### 10-Fold CV¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "sought-drain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'BERT_10Fold' does not exist. Creating a new experiment\n",
      "Fold: 1\n",
      "Epoch  1 / 5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dropout(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-7f834c453bf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m bert_history, bert_test_outputs = get_oof_and_test_preds(mlf_XP='BERT_10Fold',\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                          \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bert'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                          \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                          \u001b[0mtrain_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_train_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                          \u001b[0mtest_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-920837eebf79>\u001b[0m in \u001b[0;36mget_oof_and_test_preds\u001b[0;34m(mlf_XP, model_type, tokenizer, train_df, test_df, single_model)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}_fold_{}.bin'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         history, preds, outputs = train_fold(mlf_XP = mlf_XP, \n\u001b[0m\u001b[1;32m     67\u001b[0m                                              \u001b[0mxp_name_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_Fold'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                              \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-da653fe3e061>\u001b[0m in \u001b[0;36mtrain_fold\u001b[0;34m(mlf_XP, xp_name_iter, epochs, model, device, train_dataloader, val_dataloader, test_dataloader, loss_fn, optimizer, scheduler, model_save_name, n_train, n_val, single_model)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             training_output = train_model(model, \n\u001b[0m\u001b[1;32m     22\u001b[0m                                           \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                           \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-8a0218311ad4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, device, data_loader, loss_function, optimizer, scheduler, n_examples)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-6408bb680f7c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m         )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: dropout(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "bert_history, bert_test_outputs = get_oof_and_test_preds(mlf_XP='BERT_10Fold',\n",
    "                                                         model_type='bert', \n",
    "                                                         tokenizer=bert_tokenizer, \n",
    "                                                         train_df=df_train_full, \n",
    "                                                         test_df=df_test,\n",
    "                                                         single_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-block",
   "metadata": {},
   "source": [
    "# Essai FF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-longitude",
   "metadata": {},
   "source": [
    "source : https://huggingface.co/transformers/training.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "rubber-attachment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "determined-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "embedded-constitution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239fd19e833445e69d5c6a0729d3d09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a663951ea0b649a29d6c1a1913a82d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a3cc1c257c4b649dc5b5f221fb7bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a06dd66b2d471ea9fa2186e2124dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8717dcfe594f4a13957194e87bc32d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not paraphrase: 10%\n",
      "is paraphrase: 90%\n",
      "not paraphrase: 94%\n",
      "is paraphrase: 6%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "# The tokekenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to the sequence, as well as compute the attention masks.\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "\n",
    "paraphrase_classification_logits = model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "\n",
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "organizational-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "brilliant-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-1.8.0-py3-none-any.whl (237 kB)\n",
      "\u001b[K     |████████████████████████████████| 237 kB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 9.8 MB/s  eta 0:00:011\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 29.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp38-cp38-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 41.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.2.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (20.9)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from datasets) (0.8.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
      "Collecting tqdm<4.50.0,>=4.27\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 20.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.20.1)\n",
      "Collecting huggingface-hub<0.1.0\n",
      "  Downloading huggingface_hub-0.0.13-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: tqdm, dill, xxhash, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.56.2\n",
      "    Uninstalling tqdm-4.56.2:\n",
      "      Successfully uninstalled tqdm-4.56.2\n",
      "Successfully installed datasets-1.8.0 dill-0.3.4 huggingface-hub-0.0.13 multiprocess-0.70.12.2 tqdm-4.49.0 xxhash-2.0.2\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "floating-steal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee6c50d75b642e4b8a0422bf58e576e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c7989d8bd14861ae6973133a80f974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e69bbac17234398bd025fb7c8f33134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "strange-sacramento",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "creative-category",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f7c0806c174af8a70b8f404e1045f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9894a8fc7cfc413aa880fabc29eb32a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6ed5688f65410ab49b861cd5eba700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "supposed-airfare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label', 'text', 'token_type_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label', 'text', 'token_type_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label', 'text', 'token_type_ids'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "deadly-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "full_train_dataset = tokenized_datasets[\"train\"]\n",
    "full_eval_dataset = tokenized_datasets[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "suited-faculty",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-agenda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-birthday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-colon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-aerospace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-dover",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-syndicate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "funky-asthma",
   "metadata": {},
   "source": [
    "## roBERTa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-germany",
   "metadata": {},
   "source": [
    "Single Model¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "driven-ribbon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e495d004dc1b4989b826e11b9020d40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81435f3cb20c4cf894237379c7341d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc40aa7c8b6046e9ad6fc2dcdb7d109e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ROBERTA_MODEL_NAME = 'roberta-base'\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "peaceful-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_train_ds = create_dataset(df_train, roberta_tokenizer, MAX_LENGTH)\n",
    "roberta_test_ds = create_dataset(df_test, roberta_tokenizer, MAX_LENGTH)\n",
    "roberta_val_ds = create_dataset(df_val, roberta_tokenizer, MAX_LENGTH)\n",
    "\n",
    "roberta_train_dataloader = create_dataloader(roberta_train_ds, BATCH_SIZE)\n",
    "roberta_test_dataloader = create_dataloader(roberta_test_ds, BATCH_SIZE)\n",
    "roberta_val_dataloader = create_dataloader(roberta_val_ds, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "available-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaSentimentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(RobertaSentimentClassifier, self).__init__()\n",
    "        self.model = RobertaModel.from_pretrained(ROBERTA_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(DROPOUT_PROB)\n",
    "        self.output = nn.Linear(self.model.config.hidden_size, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        output = self.drop(pooled_output)\n",
    "        \n",
    "        return self.output(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "confidential-custody",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9f6af00a1f4d078c3e0bfd11ee76c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a0bb86c9794344985162bf85095fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_model = RobertaSentimentClassifier(n_classes=NUM_CLASSES)\n",
    "roberta_model = roberta_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "russian-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = len(roberta_train_dataloader.dataset) * EPOCHS\n",
    "\n",
    "roberta_optimizer = AdamW(roberta_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, correct_bias=True)\n",
    "roberta_scheduler = get_linear_schedule_with_warmup(roberta_optimizer, num_warmup_steps=int(0.1 * training_steps), num_training_steps=training_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "modern-hebrew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 / 5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dropout(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-cf8c4efaad82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m roberta_single_model_items = train_fold(mlf_XP='BERT', \n\u001b[0m\u001b[1;32m      2\u001b[0m                                         \u001b[0mxp_name_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BERT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroberta_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                         \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-da653fe3e061>\u001b[0m in \u001b[0;36mtrain_fold\u001b[0;34m(mlf_XP, xp_name_iter, epochs, model, device, train_dataloader, val_dataloader, test_dataloader, loss_fn, optimizer, scheduler, model_save_name, n_train, n_val, single_model)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             training_output = train_model(model, \n\u001b[0m\u001b[1;32m     22\u001b[0m                                           \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                           \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-8a0218311ad4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, device, data_loader, loss_function, optimizer, scheduler, n_examples)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-a01e98a2b00c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m         )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: dropout(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "roberta_single_model_items = train_fold(mlf_XP='BERT', \n",
    "                                        xp_name_iter='BERT',\n",
    "                                        epochs=EPOCHS,\n",
    "                                        model=roberta_model,\n",
    "                                        device=device, \n",
    "                                        train_dataloader=roberta_train_dataloader, \n",
    "                                        val_dataloader=roberta_val_dataloader,\n",
    "                                        test_dataloader=roberta_test_dataloader,\n",
    "                                        loss_fn=loss_function,\n",
    "                                        optimizer=roberta_optimizer,\n",
    "                                        scheduler=roberta_scheduler,\n",
    "                                        model_save_name='roberta_best_model.bin',\n",
    "                                        n_train=len(df_train),\n",
    "                                        n_val=len(df_val),\n",
    "                                        single_model=True\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-mainland",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
